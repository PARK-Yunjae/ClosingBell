"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API í´ë¼ì´ì–¸íŠ¸
================================

ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ì‚¬ìš©:
    from src.adapters.naver_client import get_naver_client
    client = get_naver_client()
    news = client.search_news("ì‚¼ì„±ì „ì", days=7)
"""

import os
import logging
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
import urllib.request
import urllib.parse
import json

logger = logging.getLogger(__name__)


@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„°"""
    title: str
    link: str
    description: str
    pub_date: datetime
    source: str = ""
    
    @property
    def clean_title(self) -> str:
        """HTML íƒœê·¸ ì œê±°ëœ ì œëª©"""
        import re
        return re.sub('<[^<]+?>', '', self.title).strip()
    
    @property
    def clean_description(self) -> str:
        """HTML íƒœê·¸ ì œê±°ëœ ì„¤ëª…"""
        import re
        return re.sub('<[^<]+?>', '', self.description).strip()


class NaverClient:
    """ë„¤ì´ë²„ ê²€ìƒ‰ API í´ë¼ì´ì–¸íŠ¸"""
    
    BASE_URL = "https://openapi.naver.com/v1/search/news.json"
    
    def __init__(self, client_id: str = None, client_secret: str = None):
        """
        Args:
            client_id: ë„¤ì´ë²„ API Client ID
            client_secret: ë„¤ì´ë²„ API Client Secret
        """
        self.client_id = client_id or os.getenv("NaverAPI_Client_ID")
        self.client_secret = client_secret or os.getenv("NaverAPI_Client_Secret")
        
        if not self.client_id or not self.client_secret:
            raise ValueError("Naver API credentials not found")
        
        self.call_delay = 0.1  # API í˜¸ì¶œ ê°„ê²© (ì´ˆ)
        self._last_call = 0
    
    def _make_request(self, params: Dict) -> Dict:
        """API ìš”ì²­ ì‹¤í–‰"""
        # Rate limiting
        elapsed = time.time() - self._last_call
        if elapsed < self.call_delay:
            time.sleep(self.call_delay - elapsed)
        
        # URL ìƒì„±
        query_string = urllib.parse.urlencode(params)
        url = f"{self.BASE_URL}?{query_string}"
        
        # ìš”ì²­ í—¤ë”
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", self.client_id)
        request.add_header("X-Naver-Client-Secret", self.client_secret)
        
        try:
            response = urllib.request.urlopen(request)
            self._last_call = time.time()
            
            if response.getcode() == 200:
                return json.loads(response.read().decode('utf-8'))
            else:
                logger.error(f"Naver API error: {response.getcode()}")
                return {}
                
        except Exception as e:
            logger.error(f"Naver API request failed: {e}")
            return {}
    
    def search_news(
        self,
        query: str,
        days: int = 7,
        max_results: int = 20,
        sort: str = "date",
    ) -> List[NewsArticle]:
        """ë‰´ìŠ¤ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ì–´ (ì¢…ëª©ëª… ë˜ëŠ” í‚¤ì›Œë“œ)
            days: ìµœê·¼ Nì¼ ì´ë‚´ ë‰´ìŠ¤
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
            sort: ì •ë ¬ ë°©ì‹ ('sim': ìœ ì‚¬ë„, 'date': ë‚ ì§œ)
            
        Returns:
            ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰: {query} (ìµœê·¼ {days}ì¼)")
        
        params = {
            "query": query,
            "display": min(max_results, 100),
            "start": 1,
            "sort": sort,
        }
        
        result = self._make_request(params)
        
        if not result or "items" not in result:
            logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {query}")
            return []
        
        # ë‚ ì§œ í•„í„°ë§
        cutoff_date = datetime.now() - timedelta(days=days)
        articles = []
        
        for item in result["items"]:
            try:
                # pubDate íŒŒì‹± (ì˜ˆ: "Thu, 14 Jan 2026 10:30:00 +0900")
                pub_date_str = item.get("pubDate", "")
                pub_date = self._parse_date(pub_date_str)
                
                if pub_date and pub_date >= cutoff_date:
                    article = NewsArticle(
                        title=item.get("title", ""),
                        link=item.get("originallink") or item.get("link", ""),
                        description=item.get("description", ""),
                        pub_date=pub_date,
                    )
                    articles.append(article)
                    
            except Exception as e:
                logger.debug(f"ë‰´ìŠ¤ íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {len(articles)}ê±´ (ì´ {result.get('total', 0)}ê±´ ì¤‘)")
        return articles
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        formats = [
            "%a, %d %b %Y %H:%M:%S %z",  # Thu, 14 Jan 2026 10:30:00 +0900
            "%Y-%m-%d %H:%M:%S",
            "%Y-%m-%dT%H:%M:%S",
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
        
        return None
    
    def search_stock_news(
        self,
        stock_name: str,
        stock_code: str = None,
        days: int = 7,
    ) -> List[NewsArticle]:
        """ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ (ì—¬ëŸ¬ í‚¤ì›Œë“œ ì¡°í•©)
        
        Args:
            stock_name: ì¢…ëª©ëª…
            stock_code: ì¢…ëª©ì½”ë“œ (ì„ íƒ)
            days: ìµœê·¼ Nì¼
            
        Returns:
            ì¤‘ë³µ ì œê±°ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        all_articles = []
        seen_links = set()
        
        # ê²€ìƒ‰ì–´ ì¡°í•©
        queries = [
            stock_name,
            f"{stock_name} ì£¼ê°€",
            f"{stock_name} ì‹¤ì ",
        ]
        
        if stock_code:
            queries.append(stock_code)
        
        for query in queries:
            articles = self.search_news(query, days=days, max_results=10)
            
            for article in articles:
                if article.link not in seen_links:
                    seen_links.add(article.link)
                    all_articles.append(article)
        
        # ë‚ ì§œ ì—­ìˆœ ì •ë ¬
        all_articles.sort(key=lambda x: x.pub_date, reverse=True)
        
        return all_articles[:20]  # ìµœëŒ€ 20ê±´
    
    def extract_keywords(self, articles: List[NewsArticle], top_n: int = 10) -> List[str]:
        """ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ìˆœ ë¹ˆë„ ê¸°ë°˜)
        
        Args:
            articles: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            top_n: ìƒìœ„ Nê°œ í‚¤ì›Œë“œ
            
        Returns:
            í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        from collections import Counter
        import re
        
        # ë¶ˆìš©ì–´
        stopwords = {
            'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë…„', 'ì›”', 'ì¼',
            'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì§€ë‚œ', 'ì´ë²ˆ', 'ê´€ë ¨', 'ëŒ€í•œ', 'ìœ„í•´', 'í†µí•´', 'ë”°ë¼', 'ëŒ€í•´',
            'ì¦ê¶Œ', 'ì£¼ì‹', 'íˆ¬ì', 'ì‹œì¥', 'ê¸°ì—…', 'íšŒì‚¬', 'ë§¤ìˆ˜', 'ë§¤ë„', 'ìƒìŠ¹', 'í•˜ë½',
        }
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = " ".join([
            a.clean_title + " " + a.clean_description
            for a in articles
        ])
        
        # ë‹¨ì–´ ì¶”ì¶œ (í•œê¸€ 2ê¸€ì ì´ìƒ)
        words = re.findall(r'[ê°€-í£]{2,}', text)
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ë¹ˆë„ ê³„ì‚°
        word_counts = Counter(
            word for word in words
            if word not in stopwords
        )
        
        return [word for word, _ in word_counts.most_common(top_n)]


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_client: Optional[NaverClient] = None


def get_naver_client() -> NaverClient:
    """ë„¤ì´ë²„ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _client
    if _client is None:
        _client = NaverClient()
    return _client


# ============================================================
# í…ŒìŠ¤íŠ¸
# ============================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("=" * 60)
    print("ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ API í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    try:
        client = get_naver_client()
        
        # í…ŒìŠ¤íŠ¸ 1: ë‹¨ìˆœ ê²€ìƒ‰
        print("\n[í…ŒìŠ¤íŠ¸ 1] ì‚¼ì„±ì „ì ë‰´ìŠ¤ ê²€ìƒ‰")
        news = client.search_news("ì‚¼ì„±ì „ì", days=3, max_results=5)
        for i, article in enumerate(news, 1):
            print(f"  {i}. {article.clean_title[:50]}...")
            print(f"     {article.pub_date.strftime('%Y-%m-%d %H:%M')}")
        
        # í…ŒìŠ¤íŠ¸ 2: ì¢…ëª© ë‰´ìŠ¤ ê²€ìƒ‰
        print("\n[í…ŒìŠ¤íŠ¸ 2] ì¢…ëª© ë‰´ìŠ¤ ê²€ìƒ‰ (ë³µí•©)")
        news = client.search_stock_news("ì‚¼ì„±ì „ì", "005930", days=7)
        print(f"  ì´ {len(news)}ê±´ ìˆ˜ì§‘")
        
        # í…ŒìŠ¤íŠ¸ 3: í‚¤ì›Œë“œ ì¶”ì¶œ
        print("\n[í…ŒìŠ¤íŠ¸ 3] í‚¤ì›Œë“œ ì¶”ì¶œ")
        keywords = client.extract_keywords(news, top_n=10)
        print(f"  í‚¤ì›Œë“œ: {', '.join(keywords)}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
