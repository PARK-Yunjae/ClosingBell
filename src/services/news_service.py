"""
ë‰´ìŠ¤ ìˆ˜ì§‘ ì„œë¹„ìŠ¤ v6.0
=====================

ë„¤ì´ë²„ ë‰´ìŠ¤ API + Gemini ìš”ì•½ìœ¼ë¡œ
ìœ ëª©ë¯¼ ê³µë¶€ë²• ì¢…ëª©ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

- study_date ê¸°ì¤€ Â±3ì¼ ë‰´ìŠ¤ ê²€ìƒ‰
- ì¢…ëª©ë‹¹ 10ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘
- Geminië¡œ ìš”ì•½ ë° ê°ì„± ë¶„ì„

ì‚¬ìš©:
    python main.py --run-news
    
    ë˜ëŠ” ì½”ë“œì—ì„œ:
    from src.services.news_service import run_news_collection
    run_news_collection()
"""

import os
import re
import time
import json
import logging
import urllib.request
import urllib.parse
from datetime import date, datetime, timedelta
from typing import Dict, List, Optional
from html import unescape

from src.services.http_utils import urlopen_with_retry, redact_url, mask_text

from dotenv import load_dotenv

from src.infrastructure.repository import (
    get_nomad_candidates_repository,
    get_nomad_news_repository,
)

logger = logging.getLogger(__name__)

# .env ë¡œë“œ
load_dotenv()

# API ì„¤ì •
NAVER_CLIENT_ID = os.getenv('NaverAPI_Client_ID', '')
NAVER_CLIENT_SECRET = os.getenv('NaverAPI_Client_Secret', '')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', '')

# ìƒìˆ˜
NEWS_PER_STOCK = 10      # ì¢…ëª©ë‹¹ ë‰´ìŠ¤ ìˆ˜
API_DELAY = 0.5          # API í˜¸ì¶œ ê°„ê²© (ì´ˆ)
# Gemini ìš”ì•½ ON/OFF (ë¹„ìš© ì ˆê°ìš©)
# Falseë¡œ ì„¤ì •í•˜ë©´ Gemini API í˜¸ì¶œ ì—†ì´ ë‰´ìŠ¤ ì œëª©+ìŠ¤ë‹ˆí«ë§Œ ì €ì¥
ENABLE_GEMINI_SUMMARY = False  # âš ï¸ Trueë©´ í•˜ë£¨ 10ë§Œì›+ ë¹„ìš© ë°œìƒ ê°€ëŠ¥

GEMINI_MODEL = "gemini-2.0-flash"  # 2.5-flashë³´ë‹¤ ì €ë ´ (ë¬´ë£Œ í‹°ì–´ ìˆìŒ)

# VI ê´€ë ¨ ì œì™¸ í‚¤ì›Œë“œ (ìµœê·¼ ì´ìŠˆ ì œì™¸)
EXCLUDE_KEYWORDS = [
    'VIë°œë™', 'VI ë°œë™', 'ë³€ë™ì„±ì™„í™”ì¥ì¹˜',
    'íˆ¬ìì£¼ì˜', 'íˆ¬ìê²½ê³ ', 'íˆ¬ììœ„í—˜',
    'ìƒí•œê°€', 'í•˜í•œê°€',  # ë‹¹ì¼ ê°€ê²© ë‰´ìŠ¤ ì œì™¸
]


def clean_html(text: str) -> str:
    """HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
    text = re.sub(r'<[^>]+>', '', text)
    text = unescape(text)
    text = text.replace('&quot;', '"')
    text = text.replace('&amp;', '&')
    text = text.strip()
    return text


def should_exclude_news(title: str, description: str) -> bool:
    """VI/ë‹¹ì¼ ì´ìŠˆ ë‰´ìŠ¤ ì œì™¸ ì—¬ë¶€"""
    content = (title + " " + description).lower()
    for keyword in EXCLUDE_KEYWORDS:
        if keyword.lower() in content:
            return True
    return False


def search_naver_news(query: str, display: int = 30, sort: str = 'date') -> List[Dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API
    
    Args:
        query: ê²€ìƒ‰ì–´
        display: ê²°ê³¼ ê°œìˆ˜ (ìµœëŒ€ 100)
        sort: ì •ë ¬ (date: ìµœì‹ ìˆœ, sim: ê´€ë ¨ë„ìˆœ)
        
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        logger.error("ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return []
    
    try:
        encText = urllib.parse.quote(query)
        url = f"https://openapi.naver.com/v1/search/news.json?query={encText}&display={display}&sort={sort}"
        
        request = urllib.request.Request(url)
        request.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
        request.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
        
        safe_url = redact_url(url)
        response = urlopen_with_retry(
            request,
            timeout=10,
            max_retries=2,
            backoff=1.0,
            logger=logger,
            context=f"Naver News {safe_url}",
        )
        if response is None:
            return []
        rescode = response.getcode()
        
        if rescode == 200:
            response_body = response.read()
            data = json.loads(response_body.decode('utf-8'))
            
            news_list = []
            for item in data.get('items', []):
                title = clean_html(item.get('title', ''))
                description = clean_html(item.get('description', ''))
                
                # VI/ë‹¹ì¼ ì´ìŠˆ ë‰´ìŠ¤ ì œì™¸
                if should_exclude_news(title, description):
                    continue
                
                news = {
                    'title': title,
                    'description': description,
                    'link': item.get('link', ''),
                    'originallink': item.get('originallink', ''),
                    'pub_date': item.get('pubDate', ''),
                    'source': extract_source(item.get('originallink', '') or item.get('link', '')),
                }
                news_list.append(news)
            
            return news_list
        else:
            logger.error(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {rescode}")
            return []
            
    except Exception as e:
        logger.error(f"ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {mask_text(str(e))}")
        return []


def extract_source(url: str) -> str:
    """URLì—ì„œ ë‰´ìŠ¤ ì¶œì²˜ ì¶”ì¶œ"""
    try:
        match = re.search(r'https?://([^/]+)', url)
        if match:
            domain = match.group(1)
            domain = re.sub(r'^(www\.|news\.|m\.)', '', domain)
            
            domain_map = {
                'hankyung.com': 'í•œêµ­ê²½ì œ',
                'mk.co.kr': 'ë§¤ì¼ê²½ì œ',
                'mt.co.kr': 'ë¨¸ë‹ˆíˆ¬ë°ì´',
                'edaily.co.kr': 'ì´ë°ì¼ë¦¬',
                'sedaily.com': 'ì„œìš¸ê²½ì œ',
                'fnnews.com': 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤',
                'newsis.com': 'ë‰´ì‹œìŠ¤',
                'yna.co.kr': 'ì—°í•©ë‰´ìŠ¤',
                'yonhapnews.co.kr': 'ì—°í•©ë‰´ìŠ¤',
                'chosun.com': 'ì¡°ì„ ì¼ë³´',
                'donga.com': 'ë™ì•„ì¼ë³´',
                'hani.co.kr': 'í•œê²¨ë ˆ',
                'khan.co.kr': 'ê²½í–¥ì‹ ë¬¸',
                'etnews.com': 'ì „ìì‹ ë¬¸',
                'bloter.net': 'ë¸”ë¡œí„°',
                'thebusanilbo.com': 'ë¶€ì‚°ì¼ë³´',
                'naver.com': 'ë„¤ì´ë²„ë‰´ìŠ¤',
            }
            for key, value in domain_map.items():
                if key in domain:
                    return value
            return domain
    except:
        pass
    return 'ê¸°íƒ€'


def parse_pub_date(pub_date: str) -> Optional[str]:
    """ë°œí–‰ì¼ íŒŒì‹± -> YYYY-MM-DD"""
    try:
        # "Tue, 14 Jan 2025 10:30:00 +0900" í˜•ì‹
        dt = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %z")
        return dt.strftime("%Y-%m-%d")
    except:
        return None


def summarize_with_gemini(stock_name: str, study_date: str, news_list: List[Dict]) -> List[Dict]:
    """
    Geminië¡œ ë‰´ìŠ¤ ìš”ì•½ ë° ê°ì„± ë¶„ì„
    
    Args:
        stock_name: ì¢…ëª©ëª…
        study_date: ê¸‰ë“± ë‚ ì§œ
        news_list: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ìš”ì•½ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    if not GEMINI_API_KEY:
        logger.warning("Gemini API í‚¤ê°€ ì—†ì–´ ìš”ì•½ ìƒëµ")
        for news in news_list:
            news['summary'] = news.get('description', '')[:150]
            news['sentiment'] = 'neutral'
            news['relevance'] = 0.5
        return news_list
    
    try:
        from google import genai
        client = genai.Client(api_key=GEMINI_API_KEY)
    except ImportError:
        logger.warning("google-genai íŒ¨í‚¤ì§€ ì—†ìŒ. pip install google-genai")
        for news in news_list:
            news['summary'] = news.get('description', '')[:150]
            news['sentiment'] = 'neutral'
            news['relevance'] = 0.5
        return news_list
    except Exception as e:
        logger.error(f"Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {mask_text(str(e))}")
        for news in news_list:
            news['summary'] = news.get('description', '')[:150]
            news['sentiment'] = 'neutral'
            news['relevance'] = 0.5
        return news_list
    
    # ë‰´ìŠ¤ë“¤ì„ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ë¡œ ìš”ì•½
    news_text = ""
    for i, news in enumerate(news_list[:NEWS_PER_STOCK], 1):
        news_text += f"""
[ë‰´ìŠ¤ {i}]
ì œëª©: {news['title']}
ë‚´ìš©: {news['description']}
ì¶œì²˜: {news['source']}
ë‚ ì§œ: {news.get('pub_date', '')}
---
"""
    
    prompt = f"""
'{stock_name}' ì¢…ëª©ì´ {study_date}ì— ê¸‰ë“±í–ˆìŠµë‹ˆë‹¤.
ì•„ë˜ ë‰´ìŠ¤ë“¤ì„ ë¶„ì„í•´ì„œ ê¸‰ë“± ì›ì¸ê³¼ ê´€ë ¨ì„±ì„ íŒŒì•…í•´ì£¼ì„¸ìš”.

{news_text}

ê° ë‰´ìŠ¤ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ì˜ JSON ë°°ì—´ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”:
[
  {{
    "index": 1,
    "summary": "í•µì‹¬ ë‚´ìš© 1-2ë¬¸ì¥ ìš”ì•½ (í•œêµ­ì–´)",
    "sentiment": "positive/negative/neutral",
    "relevance": 0.0~1.0,
    "category": "ì‹¤ì /í…Œë§ˆ/ì„¹í„°/ìˆ˜ê¸‰/ê³µì‹œ/ì •ì±…/ê¸°íƒ€"
  }},
  ...
]

ì£¼ì˜:
- JSONë§Œ ì¶œë ¥, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´
- sentimentëŠ” ì£¼ê°€ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ê¸°ì¤€
- relevanceëŠ” ê¸‰ë“±ê³¼ì˜ ê´€ë ¨ì„± (1.0=ì§ì ‘ì  ì›ì¸, 0.0=ë¬´ê´€)
- ë¶ˆí™•ì‹¤í•˜ë©´ relevance ë‚®ê²Œ
"""
    
    try:
        max_retries = 2
        response = None
        for attempt in range(max_retries + 1):
            try:
                # max_output_tokens ì„¤ì •ìœ¼ë¡œ JSON ì˜ë¦¼ ë°©ì§€
                response = client.models.generate_content(
                    model=GEMINI_MODEL,
                    contents=prompt,
                    config={
                        'max_output_tokens': 4096,  # ë‰´ìŠ¤ ìš”ì•½ìš© (ì—¬ìœ ìˆê²Œ)
                        'temperature': 0.3,
                    },
                )
                break
            except Exception as e:
                if attempt < max_retries:
                    wait_time = 2 ** attempt
                    logger.warning(f"Gemini ìš”ì²­ ì‹¤íŒ¨, {wait_time}ì´ˆ í›„ ì¬ì‹œë„: {mask_text(str(e))}")
                    time.sleep(wait_time)
                    continue
                raise

        if response is None:
            raise RuntimeError("Gemini ì‘ë‹µ ì—†ìŒ")
        
        result_text = response.text
        
        # JSON íŒŒì‹±
        if "```json" in result_text:
            json_str = result_text.split("```json")[1].split("```")[0]
        elif "```" in result_text:
            json_str = result_text.split("```")[1].split("```")[0]
        else:
            json_str = result_text
        
        summaries = json.loads(json_str.strip())
        
        # ê²°ê³¼ ë³‘í•©
        for summary in summaries:
            idx = summary.get('index', 0) - 1
            if 0 <= idx < len(news_list):
                news_list[idx]['summary'] = summary.get('summary', news_list[idx].get('description', '')[:150])
                news_list[idx]['sentiment'] = summary.get('sentiment', 'neutral')
                news_list[idx]['relevance'] = summary.get('relevance', 0.5)
                news_list[idx]['category'] = summary.get('category', 'ê¸°íƒ€')
        
        logger.info(f"  âœ… Gemini ìš”ì•½ ì™„ë£Œ")
        return news_list
        
    except Exception as e:
        logger.error(f"Gemini ìš”ì•½ ì‹¤íŒ¨: {mask_text(str(e))}")
        for news in news_list:
            news['summary'] = news.get('description', '')[:150]
            news['sentiment'] = 'neutral'
            news['relevance'] = 0.5
        return news_list


def collect_news_for_candidate(candidate: Dict) -> Dict:
    """
    ë‹¨ì¼ ì¢…ëª©ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘
    
    Args:
        candidate: nomad_candidates ë ˆì½”ë“œ
        
    Returns:
        ìˆ˜ì§‘ ê²°ê³¼ {'collected': int, 'saved': int}
    """
    stock_code = candidate['stock_code']
    stock_name = candidate['stock_name']
    candidate_id = candidate['id']
    study_date = candidate['study_date']
    
    logger.info(f"  ğŸ“° {stock_name} ({stock_code}) - {study_date}")
    
    result = {'collected': 0, 'saved': 0}
    
    # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ì¢…ëª©ëª… + ì£¼ì‹) - 50ê°œ ê²€ìƒ‰í•´ì„œ 10ê°œ ì„ ë³„
    query = f"{stock_name} ì£¼ì‹"
    news_list = search_naver_news(query, display=50, sort='date')
    
    if not news_list:
        logger.warning(f"  âš ï¸ {stock_name}: ë‰´ìŠ¤ ì—†ìŒ")
        # ë‰´ìŠ¤ ì—†ì–´ë„ ìˆ˜ì§‘ ì™„ë£Œ í‘œì‹œ
        candidates_repo = get_nomad_candidates_repository()
        candidates_repo.update_news_collected(candidate_id, news_count=0)
        return result
    
    # 2. study_date ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§ (Â±2ë…„ = 730ì¼)
    try:
        target_date = datetime.strptime(study_date, "%Y-%m-%d").date()
    except:
        target_date = date.today()
    
    filtered_news = []
    for news in news_list:
        news_date_str = parse_pub_date(news.get('pub_date', ''))
        if news_date_str:
            try:
                news_date = datetime.strptime(news_date_str, "%Y-%m-%d").date()
                # Â±2ë…„ ì´ë‚´ ë‰´ìŠ¤ (730ì¼)
                days_diff = abs((news_date - target_date).days)
                if days_diff <= 730:
                    news['news_date'] = news_date_str
                    news['days_diff'] = days_diff
                    filtered_news.append(news)
            except:
                pass
    
    # ë‚ ì§œ ì°¨ì´ìˆœ ì •ë ¬ (ê¸‰ë“±ì¼ì— ê°€ê¹Œìš´ ìˆœ)
    filtered_news.sort(key=lambda x: x.get('days_diff', 999))
    
    # ìµœì†Œ 10ê°œ ë³´ì¥: ë¶€ì¡±í•˜ë©´ ë‚ ì§œ í•„í„° ì—†ì´ ë³´ì¶©
    if len(filtered_news) < NEWS_PER_STOCK:
        logger.info(f"  âš ï¸ {len(filtered_news)}ê°œ < {NEWS_PER_STOCK}ê°œ, ì¶”ê°€ ê²€ìƒ‰...")
        
        # ì´ë¯¸ ìˆëŠ” URL ì œì™¸í•˜ê³  ì¶”ê°€
        existing_urls = {n.get('link', '') for n in filtered_news}
        for news in news_list:
            if len(filtered_news) >= NEWS_PER_STOCK:
                break
            if news.get('link', '') not in existing_urls:
                news['news_date'] = parse_pub_date(news.get('pub_date', '')) or study_date
                news['days_diff'] = 999  # ë‚ ì§œ ë¶ˆí™•ì‹¤
                filtered_news.append(news)
    
    filtered_news = filtered_news[:NEWS_PER_STOCK]
    
    if not filtered_news:
        logger.warning(f"  âš ï¸ {stock_name}: ë‰´ìŠ¤ ì—†ìŒ - ìµœì‹  ë‰´ìŠ¤ë¡œ ëŒ€ì²´")
        # ë‚ ì§œ í•„í„°ë§ ì—†ì´ ìµœì‹  ë‰´ìŠ¤ë¡œ ëŒ€ì²´
        filtered_news = news_list[:NEWS_PER_STOCK]
        for news in filtered_news:
            news['news_date'] = parse_pub_date(news.get('pub_date', '')) or study_date
    
    result['collected'] = len(filtered_news)
    logger.info(f"  ğŸ“¥ {len(filtered_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
    
    time.sleep(API_DELAY)
    
    # 3. Gemini ìš”ì•½ (ì„ íƒì  - ë¹„ìš© ì ˆê°)
    if ENABLE_GEMINI_SUMMARY:
        filtered_news = summarize_with_gemini(stock_name, study_date, filtered_news)
        time.sleep(API_DELAY)
    else:
        # Gemini ì—†ì´ ìŠ¤ë‹ˆí«ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
        for news in filtered_news:
            news['summary'] = news.get('snippet', '')[:300]
    
    # 4. DB ì €ì¥
    news_repo = get_nomad_news_repository()
    
    for news in filtered_news:
        try:
            news_data = {
                'study_date': study_date,
                'stock_code': stock_code,
                'news_date': news.get('news_date', study_date),
                'news_title': news.get('title', '')[:200],
                'news_source': news.get('source', ''),
                'news_url': news.get('originallink') or news.get('link', ''),
                'summary': news.get('summary', '')[:500],
            }
            
            news_repo.insert(news_data)
            result['saved'] += 1
            
        except Exception as e:
            logger.error(f"  ë‰´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    # 5. candidate ì—…ë°ì´íŠ¸ (news_collected = 1, news_count)
    candidates_repo = get_nomad_candidates_repository()
    candidates_repo.update_news_collected(candidate_id, news_count=result['saved'])
    
    logger.info(f"  âœ… {result['saved']}ê°œ ì €ì¥ ì™„ë£Œ")
    
    return result


def collect_news_for_candidates(
    target_date: Optional[date] = None,
    limit: int = 600,
) -> Dict:
    """
    ìœ ëª©ë¯¼ í›„ë³´ë“¤ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘
    
    Args:
        target_date: ëŒ€ìƒ ë‚ ì§œ (Noneì´ë©´ ë‰´ìŠ¤ ë¯¸ìˆ˜ì§‘ ì „ì²´)
        limit: ìµœëŒ€ ì¢…ëª© ìˆ˜
        
    Returns:
        ìˆ˜ì§‘ ê²°ê³¼ í†µê³„
    """
    logger.info("=" * 60)
    logger.info("ğŸ“° ìœ ëª©ë¯¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    logger.info("=" * 60)
    
    # API í‚¤ í™•ì¸
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        logger.error("âŒ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        print("\nâŒ ë„¤ì´ë²„ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        print("   .env íŒŒì¼ì— NaverAPI_Client_ID, NaverAPI_Client_Secret ì„¤ì • í•„ìš”")
        return {'error': 'no_naver_api_key'}
    
    if not GEMINI_API_KEY:
        logger.warning("âš ï¸ Gemini API í‚¤ ì—†ìŒ - ìš”ì•½ ì—†ì´ ì§„í–‰")
        print("âš ï¸ Gemini API í‚¤ ì—†ìŒ - ìš”ì•½ ì—†ì´ ì§„í–‰ë©ë‹ˆë‹¤")
    else:
        print("âœ… Gemini API: ì„¤ì •ë¨")
    
    print("âœ… ë„¤ì´ë²„ API: ì„¤ì •ë¨")
    
    candidates_repo = get_nomad_candidates_repository()
    
    # ë‰´ìŠ¤ ë¯¸ìˆ˜ì§‘ í›„ë³´ ì¡°íšŒ
    if target_date:
        candidates = candidates_repo.get_by_date(target_date.isoformat())
        candidates = [c for c in candidates if not c.get('news_collected')]
    else:
        candidates = candidates_repo.get_uncollected_news(limit=limit)
    
    if not candidates:
        logger.info("ğŸ“­ ë‰´ìŠ¤ ìˆ˜ì§‘í•  í›„ë³´ ì—†ìŒ")
        print("\nğŸ“­ ë‰´ìŠ¤ ìˆ˜ì§‘í•  í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {'total': 0, 'collected': 0, 'saved': 0}
    
    logger.info(f"ğŸ“‹ ë‰´ìŠ¤ ìˆ˜ì§‘ ëŒ€ìƒ: {len(candidates)}ê°œ ì¢…ëª©")
    print(f"\nğŸ“‹ ë‰´ìŠ¤ ìˆ˜ì§‘ ëŒ€ìƒ: {len(candidates)}ê°œ ì¢…ëª©\n")
    
    stats = {'total': len(candidates), 'collected': 0, 'saved': 0}
    
    for i, candidate in enumerate(candidates[:limit]):
        print(f"[{i+1}/{min(len(candidates), limit)}] {candidate['stock_name']} ({candidate['study_date']})")
        
        result = collect_news_for_candidate(candidate)
        
        stats['collected'] += result.get('collected', 0)
        stats['saved'] += result.get('saved', 0)
        
        time.sleep(API_DELAY)
    
    logger.info("=" * 60)
    logger.info(f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {stats['saved']}ê°œ ì €ì¥")
    logger.info("=" * 60)
    
    return stats


def run_news_collection() -> Dict:
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ìš©)
    
    ì˜¤ëŠ˜ì˜ ìœ ëª©ë¯¼ í›„ë³´ë“¤ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    return collect_news_for_candidates(limit=600)


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    )
    
    print("=" * 60)
    print("ğŸ“° ìœ ëª©ë¯¼ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    result = run_news_collection()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ ìˆ˜ì§‘ ê²°ê³¼")
    print("=" * 60)
    print(f"  ëŒ€ìƒ ì¢…ëª©: {result.get('total', 0)}ê°œ")
    print(f"  ìˆ˜ì§‘ ë‰´ìŠ¤: {result.get('collected', 0)}ê°œ")
    print(f"  ì €ì¥ ì™„ë£Œ: {result.get('saved', 0)}ê°œ")
