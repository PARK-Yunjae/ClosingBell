"""
ìœ ëª©ë¯¼ ê³µë¶€ë²• ìˆ˜ì§‘ ì„œë¹„ìŠ¤ v6.2
==============================

ìƒí•œê°€/ê±°ë˜ëŸ‰ì²œë§Œ ì¢…ëª©ì„ ìˆ˜ì§‘í•˜ì—¬
nomad_candidates í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.

v6.2 ê°œì„ :
- ê±°ë˜ëŒ€ê¸ˆ ìƒìœ„ API + ì£¼ìš” ì¢…ëª© ë³´ì™„ ë¡œì§ ì œê±°
- 16:35 data_updateì—ì„œ ìˆ˜ì§‘í•œ OHLCV CSV ê¸°ë°˜ í•„í„°ë§
- ì •í™•í•œ ê±°ë˜ëŸ‰/ìƒí•œê°€ í•„í„°ë§ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜)

ì‚¬ìš©:
    from src.services.nomad_collector import run_nomad_collection
    run_nomad_collection()
"""

import logging
import os
from datetime import date
from pathlib import Path
from typing import Dict

import pandas as pd

from src.infrastructure.repository import get_nomad_candidates_repository

logger = logging.getLogger(__name__)

# ê¸°ì¤€ê°’
LIMIT_UP_THRESHOLD = 29.5  # ìƒí•œê°€ ê¸°ì¤€ (%)
VOLUME_EXPLOSION_THRESHOLD = 10_000_000  # ê±°ë˜ëŸ‰ì²œë§Œ ê¸°ì¤€ (ì£¼)

# OHLCV ë°ì´í„° ê²½ë¡œ
OHLCV_DIR = Path(os.getenv('OHLCV_DIR', 'C:/Coding/data/ohlcv'))
STOCK_MAPPING_PATH = Path(os.getenv('STOCK_MAPPING', 'C:/Coding/data/stock_mapping.csv'))

# ETF ë“± ì œì™¸ íŒ¨í„´
EXCLUDE_PATTERNS = [
    'KODEX', 'TIGER', 'KBSTAR', 'ARIRANG', 'HANARO',
    'SOL', 'KOSEF', 'KINDEX', 'SMART', 'ACE', 'TIMEFOLIO',
    'ETF', 'ETN', 'ì¸ë²„ìŠ¤', 'ë ˆë²„ë¦¬ì§€', 'ì„ ë¬¼', 'ìŠ¤íŒ©',
]


def load_stock_mapping() -> Dict[str, str]:
    """ì¢…ëª©ì½”ë“œ â†’ ì¢…ëª©ëª… ë§¤í•‘ ë¡œë“œ"""
    mapping = {}
    
    if STOCK_MAPPING_PATH.exists():
        try:
            df = pd.read_csv(STOCK_MAPPING_PATH, dtype={'code': str})
            for _, row in df.iterrows():
                code = str(row['code']).zfill(6)
                mapping[code] = row['name']
        except Exception as e:
            logger.warning(f"stock_mapping.csv ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return mapping


def collect_nomad_candidates(target_date: date = None) -> Dict:
    """
    ìœ ëª©ë¯¼ ê³µë¶€ í›„ë³´ ìˆ˜ì§‘ (CSV ê¸°ë°˜)
    
    16:35ì— ìˆ˜ì§‘ëœ OHLCV CSV íŒŒì¼ì„ ë¶„ì„í•˜ì—¬
    ê±°ë˜ëŸ‰ 1ì²œë§Œ ì´ìƒ OR ìƒí•œê°€(+29.5%) ì¢…ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        target_date: ìˆ˜ì§‘ ë‚ ì§œ (ê¸°ë³¸: ì˜¤ëŠ˜)
        
    Returns:
        {'limit_up': int, 'volume_explosion': int, 'total': int}
    """
    if target_date is None:
        target_date = date.today()
    
    target_date_str = target_date.isoformat()
    logger.info(f"ğŸ“š ìœ ëª©ë¯¼ í›„ë³´ ìˆ˜ì§‘ (CSV ê¸°ë°˜): {target_date}")
    
    repo = get_nomad_candidates_repository()
    
    # ê¸°ì¡´ ë°ì´í„° í™•ì¸
    existing = repo.get_by_date(target_date_str)
    if existing:
        logger.info(f"  ì´ë¯¸ {len(existing)}ê°œ í›„ë³´ê°€ ìˆìŒ â†’ ìŠ¤í‚µ")
        return {'limit_up': 0, 'volume_explosion': 0, 'total': len(existing), 'skipped': True}
    
    result = {'limit_up': 0, 'volume_explosion': 0, 'total': 0}
    candidates = []
    
    # ì¢…ëª©ëª… ë§¤í•‘ ë¡œë“œ
    stock_mapping = load_stock_mapping()
    logger.info(f"  ì¢…ëª© ë§¤í•‘: {len(stock_mapping)}ê°œ")
    
    # OHLCV í´ë” í™•ì¸
    if not OHLCV_DIR.exists():
        logger.error(f"  OHLCV í´ë” ì—†ìŒ: {OHLCV_DIR}")
        return result
    
    csv_files = list(OHLCV_DIR.glob("*.csv"))
    logger.info(f"  CSV íŒŒì¼: {len(csv_files)}ê°œ ìŠ¤ìº”")
    
    for csv_file in csv_files:
        try:
            stock_code = csv_file.stem  # íŒŒì¼ëª… = ì¢…ëª©ì½”ë“œ
            
            # ì¢…ëª©ëª… ì¡°íšŒ
            stock_name = stock_mapping.get(stock_code, stock_code)
            
            # ETF ë“± ì œì™¸
            skip = False
            for pattern in EXCLUDE_PATTERNS:
                if pattern.lower() in stock_name.lower():
                    skip = True
                    break
            
            if skip:
                continue
            
            # CSV ì½ê¸°
            df = pd.read_csv(csv_file)
            
            # ì»¬ëŸ¼ëª… ì†Œë¬¸ì í†µì¼
            df.columns = df.columns.str.lower()
            
            # date ì»¬ëŸ¼ í™•ì¸
            if 'date' not in df.columns:
                if 'unnamed: 0' in df.columns:
                    df = df.rename(columns={'unnamed: 0': 'date'})
                else:
                    continue
            
            # ì˜¤ëŠ˜ ë°ì´í„° ì°¾ê¸°
            df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')
            today_df = df[df['date'] == target_date_str]
            
            if today_df.empty:
                continue
            
            today_row = today_df.iloc[-1]
            
            # ë°ì´í„° ì¶”ì¶œ
            volume = int(today_row.get('volume', 0))
            close = int(today_row.get('close', 0))
            
            # ì „ì¼ ë°ì´í„°ë¡œ ë“±ë½ë¥  ê³„ì‚°
            prev_df = df[df['date'] < target_date_str]
            if prev_df.empty:
                continue
            
            prev_row = prev_df.iloc[-1]
            prev_close = int(prev_row.get('close', 0))
            
            if prev_close == 0:
                continue
            
            change_rate = ((close - prev_close) / prev_close) * 100
            
            # ê±°ë˜ëŒ€ê¸ˆ ê³„ì‚° (ì–µì›)
            trading_value = (close * volume) / 100_000_000
            
            # ìƒí•œê°€ í™•ì¸ (ë“±ë½ë¥  >= 29.5%)
            is_limit_up = change_rate >= LIMIT_UP_THRESHOLD
            
            # ê±°ë˜ëŸ‰ì²œë§Œ í™•ì¸
            is_volume_explosion = volume >= VOLUME_EXPLOSION_THRESHOLD
            
            # í•„í„°ë§: ìƒí•œê°€ OR ê±°ë˜ëŸ‰ì²œë§Œ
            if not (is_limit_up or is_volume_explosion):
                continue
            
            # ì‚¬ìœ  ê²°ì •
            if is_limit_up and is_volume_explosion:
                reason = 'ìƒí•œê°€+ê±°ë˜ëŸ‰'
            elif is_limit_up:
                reason = 'ìƒí•œê°€'
            else:
                reason = 'ê±°ë˜ëŸ‰ì²œë§Œ'
            
            candidate_data = {
                'study_date': target_date_str,
                'stock_code': stock_code,
                'stock_name': stock_name,
                'reason_flag': reason,
                'close_price': close,
                'change_rate': round(change_rate, 2),
                'volume': volume,
                'trading_value': round(trading_value, 2),
                'data_source': 'backfill',  # CSVì—ì„œ ìˆ˜ì§‘ = backfill
            }
            
            candidates.append(candidate_data)
            
            if is_limit_up:
                result['limit_up'] += 1
                logger.info(f"  ìƒí•œê°€: {stock_name} ({stock_code}) +{change_rate:.1f}%")
            if is_volume_explosion:
                result['volume_explosion'] += 1
                logger.info(f"  ê±°ë˜ëŸ‰ì²œë§Œ: {stock_name} ({stock_code}) +{change_rate:.1f}% ê±°ë˜ëŸ‰:{volume:,}")
            
        except Exception as e:
            logger.debug(f"  {csv_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    # DB ì €ì¥
    saved = 0
    for candidate in candidates:
        try:
            repo.insert(candidate)
            saved += 1
        except Exception as e:
            logger.debug(f"  ì €ì¥ ì‹¤íŒ¨ ({candidate['stock_code']}): {e}")
    
    result['total'] = saved
    
    logger.info(f"ğŸ“š ìœ ëª©ë¯¼ ìˆ˜ì§‘ ì™„ë£Œ: ìƒí•œê°€ {result['limit_up']}, ê±°ë˜ëŸ‰ì²œë§Œ {result['volume_explosion']}, ì´ {saved}ê°œ ì €ì¥")
    
    return result


def run_nomad_collection() -> Dict:
    """ìœ ëª©ë¯¼ ê³µë¶€ë²• ìˆ˜ì§‘ ì‹¤í–‰ (ìŠ¤ì¼€ì¤„ëŸ¬ìš©)"""
    logger.info("=" * 50)
    logger.info("ğŸ“š ìœ ëª©ë¯¼ ê³µë¶€ë²• ìˆ˜ì§‘ ì‹œì‘ (CSV ê¸°ë°˜)")
    logger.info("=" * 50)
    
    result = collect_nomad_candidates()
    
    logger.info("=" * 50)
    logger.info(f"ğŸ“š ìœ ëª©ë¯¼ ê³µë¶€ë²• ì™„ë£Œ: {result.get('total', 0)}ê°œ ì¢…ëª©")
    logger.info("=" * 50)
    
    return result


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    )
    
    print("=" * 60)
    print("ğŸ“š ìœ ëª©ë¯¼ ê³µë¶€ë²• í…ŒìŠ¤íŠ¸ (CSV ê¸°ë°˜)")
    print("=" * 60)
    
    result = run_nomad_collection()
    
    print()
    print(f"ìƒí•œê°€: {result.get('limit_up', 0)}ê°œ")
    print(f"ê±°ë˜ëŸ‰ì²œë§Œ: {result.get('volume_explosion', 0)}ê°œ")
    print(f"ì´: {result.get('total', 0)}ê°œ")
