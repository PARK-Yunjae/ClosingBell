"""
ì¢…ê°€ë§¤ë§¤ TOP5 AI ë¶„ì„ ì„œë¹„ìŠ¤ v7.0

ë‰´ìŠ¤ëŠ” ë©”ëª¨ë¦¬ì—ì„œë§Œ ì‚¬ìš©í•˜ê³  DB ì €ì¥ ì•ˆ í•¨
AI ë¶„ì„ ê²°ê³¼(summary, risk_level, recommendation)ë§Œ DB ì €ì¥

v6.5.1 ë³€ê²½ì‚¬í•­:
- PER/PBR ì—†ì„ ë•Œ í…Œë§ˆÂ·ìˆ˜ê¸‰ ì¤‘ì‹¬ ì¢…ëª©ìœ¼ë¡œ ë¶„ë¥˜
- ë°¸ë¥˜ì—ì´ì…˜ ì»¨í…ìŠ¤íŠ¸ ê°œì„ 
"""

import os
import json
import logging
import requests
from typing import Dict, List, Optional, Tuple
from datetime import date
from bs4 import BeautifulSoup

from src.config.settings import settings
from src.services.http_utils import request_with_retry, redact_url, mask_text

from src.utils.news_utils import fetch_news_headlines

logger = logging.getLogger(__name__)


def format_market_cap(market_cap: float) -> str:
    """ì‹œê°€ì´ì•¡ í¬ë§·"""
    if not market_cap:
        return "-"
    if market_cap >= 10000:
        return f"{market_cap/10000:.1f}ì¡°"
    return f"{market_cap:,.0f}ì–µ"


def format_volume(volume: int) -> str:
    """ê±°ë˜ëŸ‰ í¬ë§· (ë§Œì£¼ ë‹¨ìœ„)"""
    if not volume:
        return "-"
    if volume >= 100_000_000:  # 1ì–µì£¼ ì´ìƒ
        return f"{volume/100_000_000:.1f}ì–µì£¼"
    elif volume >= 10_000:  # 1ë§Œì£¼ ì´ìƒ
        return f"{volume/10_000:.0f}ë§Œì£¼"
    else:
        return f"{volume:,}ì£¼"


def format_valuation_for_top5(per, pbr) -> str:
    """TOP5ìš© ë°¸ë¥˜ì—ì´ì…˜ ì»¨í…ìŠ¤íŠ¸ (v6.5.1)"""
    has_per = per is not None and per > 0
    has_pbr = pbr is not None and pbr > 0
    
    if not has_per and not has_pbr:
        return "PER/PBR: ë¯¸ì œê³µ (ì ì ë˜ëŠ” ì‹ ê·œìƒì¥) â†’ í…Œë§ˆÂ·ìˆ˜ê¸‰ ì¤‘ì‹¬ ì¢…ëª©"
    
    parts = []
    if has_per:
        parts.append(f"PER: {per:.1f}")
    else:
        parts.append("PER: ì ì")
    
    if has_pbr:
        parts.append(f"PBR: {pbr:.1f}")
    else:
        parts.append("PBR: ë¯¸ì œê³µ")
    
    return " | ".join(parts)


def fetch_naver_company_info(stock_code: str) -> Dict:
    """ë„¤ì´ë²„ ê¸ˆìœµì—ì„œ ê¸°ì—…ì •ë³´ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬)"""
    try:
        url = f"https://finance.naver.com/item/main.naver?code={stock_code}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = request_with_retry(
            "GET",
            url,
            headers=headers,
            timeout=10,
            max_retries=2,
            backoff=1.0,
            logger=logger,
            context=f"Naver Finance {redact_url(url)}",
        )
        if resp is None:
            return {}
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        info = {}
        
        # ì‹œê°€ì´ì•¡
        market_cap_elem = soup.select_one('#_market_sum')
        if market_cap_elem:
            text = market_cap_elem.get_text(strip=True).replace(',', '').replace('ì–µì›', '')
            try:
                info['market_cap'] = float(text)
            except:
                pass
        
        # PER, PBR ë“±
        table = soup.select_one('table.per_table')
        if table:
            for row in table.select('tr'):
                cells = row.select('td, th')
                for i, cell in enumerate(cells):
                    text = cell.get_text(strip=True)
                    if 'PER' in text and i + 1 < len(cells):
                        try:
                            info['per'] = float(cells[i+1].get_text(strip=True).replace(',', ''))
                        except:
                            pass
                    if 'PBR' in text and i + 1 < len(cells):
                        try:
                            info['pbr'] = float(cells[i+1].get_text(strip=True).replace(',', ''))
                        except:
                            pass
        
        # ì—…ì¢…
        sector_elem = soup.select_one('div.sub_section h4 a')
        if sector_elem:
            info['sector'] = sector_elem.get_text(strip=True)
        
        return info
    except Exception as e:
        logger.warning(f"ê¸°ì—…ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ({stock_code}): {mask_text(str(e))}")
        return {}


def fetch_news_headlines(stock_name: str, limit: int = 5) -> List[Dict]:
    """ë„¤ì´ë²„ ê²€ìƒ‰ APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ë§Œ, DB ì €ì¥ X)"""
    try:
        from src.services.news_service import search_naver_news
        
        query = f"{stock_name} ì£¼ì‹"
        news_list = search_naver_news(query, display=limit, sort='date')
        
        if not news_list:
            return []
        
        # í˜•ì‹ ë§ì¶”ê¸°
        result = []
        for news in news_list[:limit]:
            result.append({
                'title': news.get('title', '').replace('<b>', '').replace('</b>', ''),
                'url': news.get('link', '')
            })
        
        return result
    except Exception as e:
        logger.warning(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ ({stock_name}): {e}")
        return []


def generate_top5_ai_analysis(stock_data: Dict, company_info: Dict, news_list: List[Dict]) -> Tuple[Optional[Dict], Optional[str]]:
    """
    Geminië¡œ TOP5 ì¢…ëª© AI ë¶„ì„
    
    Returns:
        (result_dict, error_message)
    """
    try:
        from google import genai
        from dotenv import load_dotenv
        
        load_dotenv()
        api_key = os.getenv('GEMINI_API_KEY') or os.getenv('GOOGLE_API_KEY')
        
        if not api_key:
            return None, "Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        client = genai.Client(api_key=api_key)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        per = company_info.get('per')
        pbr = company_info.get('pbr')
        valuation_text = format_valuation_for_top5(per, pbr)
        
        stock_info = f"""
ì¢…ëª©: {stock_data['stock_name']} ({stock_data['stock_code']})
ì ìˆ˜: {stock_data.get('screen_score', 0):.1f}ì  ({stock_data.get('grade', '-')}ë“±ê¸‰)
ë“±ë½ë¥ : {stock_data.get('change_rate', 0):+.1f}%

ì—…ì¢…: {stock_data.get('sector') or company_info.get('sector', '-')}
ì‹œê°€ì´ì•¡: {format_market_cap(company_info.get('market_cap') or stock_data.get('market_cap'))}
{valuation_text}

CCI: {stock_data.get('cci', 0):.0f}
RSI: {stock_data.get('rsi', '-')}
ì´ê²©ë„(20): {stock_data.get('disparity_20', 0):.1f}%
ì—°ì†ì–‘ë´‰: {stock_data.get('consecutive_up', 0)}ì¼
ê±°ë˜ëŒ€ê¸ˆ: {format_market_cap(stock_data.get('trading_value', 0))}
ê±°ë˜ëŸ‰: {format_volume(stock_data.get('volume', 0))}
"""
        
        news_text = ""
        if news_list:
            news_text = "\nìµœê·¼ ë‰´ìŠ¤:\n"
            for news in news_list[:5]:
                news_text += f"- {news.get('title', '')}\n"
        
        prompt = f"""
ë‹¤ìŒ ì¢…ëª©ì— ëŒ€í•´ ì¢…ê°€ë§¤ë§¤ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”.
íŠ¹íˆ ì •ë¦¬ë§¤ë§¤, ìƒì¥íì§€, íš¡ë ¹, ë¶„ì‹íšŒê³„ ë“± ìœ„í—˜ ìš”ì†Œê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.

âš ï¸ PER/PBRì´ ì—†ëŠ” ì¢…ëª©ì€:
- ì ì ê¸°ì—…ì´ê±°ë‚˜ ì‹ ê·œìƒì¥ ì¢…ëª©ì…ë‹ˆë‹¤
- ì‹¤ì  ê¸°ë°˜ ë°¸ë¥˜ì—ì´ì…˜ì´ ì–´ë ¤ìš°ë¯€ë¡œ í…Œë§ˆÂ·ìˆ˜ê¸‰Â·ëª¨ë©˜í…€ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”
- "PERì´ ì œê³µë˜ì§€ ì•Šì•„ íŒë‹¨ ì–´ë µë‹¤"ê°€ ì•„ë‹ˆë¼, í…Œë§ˆÂ·ìˆ˜ê¸‰ ì¢…ëª©ìœ¼ë¡œ ë‹¤ë¥¸ ê´€ì ì˜ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”

{stock_info}
{news_text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "summary": "í•µì‹¬ ìš”ì•½ (1-2ë¬¸ì¥)",
    "price_reason": "ì˜¤ëŠ˜ ì£¼ê°€ ê¸‰ë“± ì›ì¸ ì¶”ì • (1ë¬¸ì¥)",
    "investment_points": ["íˆ¬ì í¬ì¸íŠ¸ 1", "íˆ¬ì í¬ì¸íŠ¸ 2"],
    "risk_factors": ["ë¦¬ìŠ¤í¬ 1", "ë¦¬ìŠ¤í¬ 2"],
    "valuation_comment": "ë°¸ë¥˜ì—ì´ì…˜ ì˜ê²¬ (PER/PBR ì—†ìœ¼ë©´ í…Œë§ˆÂ·ìˆ˜ê¸‰ ê´€ì  ì½”ë©˜íŠ¸)",
    "risk_level": "ë‚®ìŒ/ë³´í†µ/ë†’ìŒ ì¤‘ í•˜ë‚˜",
    "recommendation": "ë§¤ìˆ˜/ê´€ë§/ë§¤ë„ ì¤‘ í•˜ë‚˜"
}}

ì¤‘ìš”: ì •ë¦¬ë§¤ë§¤, ê´€ë¦¬ì¢…ëª©, ìƒì¥íì§€ ìœ„í—˜ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ risk_levelì„ "ë†’ìŒ"ìœ¼ë¡œ, recommendationì„ "ë§¤ë„"ë¡œ ì„¤ì •í•˜ì„¸ìš”.
"""
        
        # settingsì—ì„œ AI ì„¤ì • ë¡œë“œ
        response = client.models.generate_content(
            model=settings.ai.model,
            contents=prompt,
            config={
                'max_output_tokens': settings.ai.max_output_tokens,
                'temperature': settings.ai.temperature,
            },
        )
        result_text = response.text
        
        # JSON íŒŒì‹±
        if '```json' in result_text:
            result_text = result_text.split('```json')[1].split('```')[0]
        elif '```' in result_text:
            result_text = result_text.split('```')[1].split('```')[0]
        
        result = json.loads(result_text.strip())
        return result, None
        
    except ImportError:
        return None, "google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    except json.JSONDecodeError as e:
        return None, f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}"
    except Exception as e:
        return None, f"AI ë¶„ì„ ì‹¤íŒ¨: {e}"


def analyze_top5_stocks(target_date: str = None, limit: int = 5) -> Dict:
    """
    ì¢…ê°€ë§¤ë§¤ TOP5 AI ë¶„ì„ ì‹¤í–‰
    
    Args:
        target_date: ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ (Noneì´ë©´ ìµœì‹ )
        limit: ë¶„ì„í•  ì¢…ëª© ìˆ˜
    
    Returns:
        {'analyzed': n, 'failed': m, 'errors': [...]}
    """
    from src.infrastructure.database import get_database
    
    db = get_database()
    stats = {'analyzed': 0, 'failed': 0, 'skipped': 0, 'errors': []}
    
    try:
        # ëŒ€ìƒ ë‚ ì§œ ê²°ì •
        if not target_date:
            cursor = db.execute("SELECT MAX(screen_date) FROM closing_top5_history")
            row = cursor.fetchone()
            target_date = row[0] if row else None
        
        if not target_date:
            logger.warning("ë¶„ì„í•  TOP5 ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return stats
        
        logger.info(f"=" * 60)
        logger.info(f"ğŸ¤– ì¢…ê°€ë§¤ë§¤ TOP5 AI ë¶„ì„ ì‹œì‘: {target_date}")
        logger.info(f"=" * 60)
        
        # TOP5 ì¡°íšŒ (AI ë¶„ì„ ì—†ëŠ” ê²ƒë§Œ)
        cursor = db.execute("""
            SELECT id, stock_code, stock_name, screen_score, grade,
                   cci, rsi, change_rate, disparity_20, consecutive_up,
                   volume_ratio_5, sector, trading_value, volume,
                   ai_summary, ai_recommendation
            FROM closing_top5_history
            WHERE screen_date = ?
            ORDER BY rank
            LIMIT ?
        """, (target_date, limit))
        
        stocks = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        
        for row in stocks:
            stock = dict(zip(columns, row))
            
            # ì´ë¯¸ ë¶„ì„ëœ ì¢…ëª© ìŠ¤í‚µ (JSON í˜•ì‹ì˜ ai_summaryê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ)
            ai_summary_raw = stock.get('ai_summary', '')
            
            # JSON í˜•ì‹ì¸ì§€ í™•ì¸ (ì›¹í›…ìš© ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì¬ë¶„ì„)
            is_valid_json = False
            if ai_summary_raw and ai_summary_raw.strip():
                try:
                    parsed = json.loads(ai_summary_raw)
                    # JSONì´ê³  í•„ìˆ˜ í•„ë“œê°€ ìˆìœ¼ë©´ ìœ íš¨
                    if isinstance(parsed, dict) and parsed.get('summary'):
                        is_valid_json = True
                except json.JSONDecodeError:
                    pass  # JSON ì•„ë‹˜ â†’ ì¬ë¶„ì„ í•„ìš”
            
            if is_valid_json:
                logger.info(f"  â­ï¸ {stock['stock_name']} - ì´ë¯¸ ë¶„ì„ë¨ (ìŠ¤í‚µ)")
                stats['skipped'] += 1
                continue
            
            logger.info(f"  ğŸ” {stock['stock_name']} ({stock['stock_code']}) ë¶„ì„ ì¤‘...")
            
            # 1. ë„¤ì´ë²„ ê¸°ì—…ì •ë³´ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬)
            company_info = fetch_naver_company_info(stock['stock_code'])
            
            # 2. ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬ë§Œ, API ì‚¬ìš©)
            news_list = fetch_news_headlines(stock['stock_name'], limit=5)
            logger.info(f"     ë‰´ìŠ¤ {len(news_list)}ê°œ ìˆ˜ì§‘ (ë©”ëª¨ë¦¬)")
            
            # 3. AI ë¶„ì„
            result, error = generate_top5_ai_analysis(stock, company_info, news_list)
            
            if error:
                logger.error(f"     âŒ AI ë¶„ì„ ì‹¤íŒ¨: {error}")
                stats['failed'] += 1
                stats['errors'].append(f"{stock['stock_name']}: {error}")
                continue
            
            # 4. DBì— AI ê²°ê³¼ë§Œ ì €ì¥
            ai_summary = json.dumps(result, ensure_ascii=False)
            ai_risk_level = result.get('risk_level', 'ë³´í†µ')
            ai_recommendation = result.get('recommendation', 'ê´€ë§')
            
            db.execute("""
                UPDATE closing_top5_history
                SET ai_summary = ?, ai_risk_level = ?, ai_recommendation = ?
                WHERE id = ?
            """, (ai_summary, ai_risk_level, ai_recommendation, stock['id']))
            
            risk_emoji = {'ë‚®ìŒ': 'âœ…', 'ë³´í†µ': 'âš ï¸', 'ë†’ìŒ': 'ğŸš«'}.get(ai_risk_level, 'â“')
            logger.info(f"     âœ… ë¶„ì„ ì™„ë£Œ: {ai_risk_level} {risk_emoji} / {ai_recommendation}")
            stats['analyzed'] += 1
        
        logger.info(f"=" * 60)
        logger.info(f"ğŸ¤– TOP5 AI ë¶„ì„ ì™„ë£Œ: {stats['analyzed']}ê°œ ì„±ê³µ, {stats['failed']}ê°œ ì‹¤íŒ¨, {stats['skipped']}ê°œ ìŠ¤í‚µ")
        logger.info(f"=" * 60)
        
        return stats
        
    except Exception as e:
        logger.error(f"TOP5 AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        stats['errors'].append(str(e))
        return stats


def run_top5_ai_analysis() -> Dict:
    """ìŠ¤ì¼€ì¤„ëŸ¬/CLIìš© ì‹¤í–‰ í•¨ìˆ˜ (ìµœì‹  1ì¼)"""
    return analyze_top5_stocks()


def run_top5_ai_analysis_all(limit_per_day: int = 5) -> Dict:
    """ë°±í•„ìš© - ì „ì²´ ë¯¸ë¶„ì„ TOP5 AI ë¶„ì„"""
    from src.infrastructure.database import get_database
    
    db = get_database()
    total_stats = {'analyzed': 0, 'failed': 0, 'skipped': 0, 'errors': []}
    
    # ë¯¸ë¶„ì„ ë‚ ì§œ ì¡°íšŒ (ai_summaryê°€ NULLì´ê±°ë‚˜ JSON ì•„ë‹Œ ì§§ì€ í…ìŠ¤íŠ¸)
    # JSONì€ '{'ë¡œ ì‹œì‘í•˜ë¯€ë¡œ, '{'ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì¬ë¶„ì„ ëŒ€ìƒ
    rows = db.fetch_all("""
        SELECT DISTINCT screen_date 
        FROM closing_top5_history 
        WHERE ai_summary IS NULL 
           OR (ai_summary NOT LIKE '{%' AND ai_summary != '')
        ORDER BY screen_date
    """)
    
    dates = [row['screen_date'] for row in rows]
    
    if not dates:
        logger.info("âœ… ëª¨ë“  TOP5ê°€ ì´ë¯¸ AI ë¶„ì„ ì™„ë£Œë¨")
        return total_stats
    
    logger.info(f"ğŸ“… ë¯¸ë¶„ì„ ë‚ ì§œ: {len(dates)}ì¼")
    
    for i, target_date in enumerate(dates):
        logger.info(f"[{i+1}/{len(dates)}] {target_date} ë¶„ì„ ì¤‘...")
        
        result = analyze_top5_stocks(target_date=target_date, limit=limit_per_day)
        
        total_stats['analyzed'] += result.get('analyzed', 0)
        total_stats['failed'] += result.get('failed', 0)
        total_stats['skipped'] += result.get('skipped', 0)
        total_stats['errors'].extend(result.get('errors', []))
    
    logger.info(f"ğŸ¤– ì „ì²´ TOP5 AI ë¶„ì„ ì™„ë£Œ: {total_stats['analyzed']}ê°œ ì„±ê³µ")
    return total_stats


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
    result = run_top5_ai_analysis()
    print(f"ê²°ê³¼: {result}")
