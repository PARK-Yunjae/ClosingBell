"""
OHLCV + ê¸€ë¡œë²Œ ë°ì´í„° ìë™ ê°±ì‹  (data_updater.py) v7.0
"""

import logging
import time
from pathlib import Path
from datetime import datetime, date, timedelta
from typing import Optional, List
import pandas as pd

from src.adapters.kiwoom_rest_client import get_kiwoom_client

logger = logging.getLogger(__name__)

# ============================================
# ì„¤ì •
# ============================================
from src.config.app_config import OHLCV_FULL_DIR as DATA_DIR, GLOBAL_DIR, MAPPING_FILE

API_DELAY = 0.3
MAX_STOCKS_PER_RUN = 3000  # v5.2: API ì œí•œ ì—¬ìœ ìˆìœ¼ë¯€ë¡œ ì „ì²´ ê°±ì‹ 

# ê³µíœ´ì¼
HOLIDAYS_2025_2026 = {
    date(2025, 1, 1), date(2025, 1, 28), date(2025, 1, 29), date(2025, 1, 30),
    date(2025, 3, 1), date(2025, 5, 5), date(2025, 5, 6), date(2025, 6, 6),
    date(2025, 8, 15), date(2025, 10, 3), date(2025, 10, 5), date(2025, 10, 6),
    date(2025, 10, 7), date(2025, 10, 8), date(2025, 10, 9), date(2025, 12, 25),
    date(2026, 1, 1),
}

def is_market_open(check_date: date = None) -> bool:
    """ì¥ ìš´ì˜ì¼ ì²´í¬"""
    if check_date is None:
        check_date = date.today()
    if check_date.weekday() >= 5:
        return False
    if check_date in HOLIDAYS_2025_2026:
        return False
    return True


def get_last_date_in_csv(file_path: Path) -> Optional[date]:
    """CSV íŒŒì¼ì˜ ë§ˆì§€ë§‰ ê±°ë˜ì¼ ë°˜í™˜"""
    try:
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        # date ì»¬ëŸ¼ ì°¾ê¸°
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            return df['date'].max().date()
        
        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œì¸ ê²½ìš° (Unnamed: 0 ë˜ëŠ” ì¸ë±ìŠ¤)
        first_col = df.columns[0]
        if first_col in ['', 'Unnamed: 0'] or 'date' in first_col.lower():
            df[first_col] = pd.to_datetime(df[first_col])
            return df[first_col].max().date()
        
        # ì¸ë±ìŠ¤ê°€ ë‚ ì§œì¸ ê²½ìš°
        df = pd.read_csv(file_path, index_col=0, parse_dates=True)
        if len(df) == 0:
            return None
        return df.index[-1].date()
        
    except Exception as e:
        logger.warning(f"CSV ì½ê¸° ì‹¤íŒ¨ {file_path.name}: {e}")
        return None


def get_business_days_between(start: date, end: date) -> int:
    """ë‘ ë‚ ì§œ ì‚¬ì´ì˜ ì˜ì—…ì¼ ìˆ˜"""
    days = (end - start).days
    weeks = days // 7
    remainder = days % 7
    return weeks * 5 + min(remainder, 5)


def load_csv_with_date(file_path: Path) -> Optional[pd.DataFrame]:
    """CSV íŒŒì¼ ë¡œë“œ (date ì»¬ëŸ¼/ì¸ë±ìŠ¤ ëª¨ë‘ ì§€ì›)
    
    Returns:
        DataFrame with date as index, lowercase columns
    """
    try:
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        
        # ì»¬ëŸ¼ëª… ì†Œë¬¸ì í†µì¼
        df.columns = df.columns.str.lower()
        
        # date ì»¬ëŸ¼ ì°¾ê¸°
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            df = df.set_index('date')
        else:
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œì¸ ê²½ìš°
            first_col = df.columns[0]
            if first_col in ['', 'unnamed: 0']:
                df = df.rename(columns={first_col: 'date'})
                df['date'] = pd.to_datetime(df['date'])
                df = df.set_index('date')
            else:
                # ì¸ë±ìŠ¤ë¡œ ë‹¤ì‹œ ì½ê¸°
                df = pd.read_csv(file_path, index_col=0, parse_dates=True, encoding='utf-8-sig')
                df.columns = df.columns.str.lower()
        
        df.index.name = 'date'
        return df
        
    except Exception as e:
        logger.warning(f"CSV ë¡œë“œ ì‹¤íŒ¨ {file_path.name}: {e}")
        return None


def update_single_stock(code: str, last_date: date, today: date) -> bool:
    """ë‹¨ì¼ ì¢…ëª© ë°ì´í„° ê°±ì‹ """
    try:
        client = get_kiwoom_client()
        days_needed = get_business_days_between(last_date, today) + 5
        prices = client.get_daily_prices(code, count=min(days_needed, 100))
        
        if not prices:
            logger.warning(f"  {code}: ë°ì´í„° ì—†ìŒ")
            return False
        
        file_path = DATA_DIR / f"{code}.csv"
        df_existing = load_csv_with_date(file_path)
        
        if df_existing is None:
            logger.warning(f"  {code}: ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        new_rows = []
        for price in prices:
            price_date = pd.Timestamp(price.date)
            if price_date not in df_existing.index and price.date > last_date:
                new_rows.append({
                    'date': price_date,
                    'open': price.open,
                    'high': price.high,
                    'low': price.low,
                    'close': price.close,
                    'volume': price.volume,
                    'trading_value': price.trading_value,
                })
        
        if new_rows:
            df_new = pd.DataFrame(new_rows)
            df_new.set_index('date', inplace=True)
            
            df_combined = pd.concat([df_existing, df_new])
            df_combined.sort_index(inplace=True)
            df_combined = df_combined[~df_combined.index.duplicated(keep='last')]
            
            # Aì•ˆ: OHLCVë§Œ ì €ì¥ (trading_valueëŠ” ê³„ì‚° ê°€ëŠ¥)
            keep_cols = ['open', 'high', 'low', 'close', 'volume']
            save_cols = [c for c in keep_cols if c in df_combined.columns]
            df_combined = df_combined[save_cols]
            
            df_combined.to_csv(file_path, index_label='date')
            logger.info(f"  âœ“ {code}: {len(new_rows)}ì¼ ì¶”ê°€ (ë§ˆì§€ë§‰: {df_combined.index[-1].date()})")
            return True
        else:
            logger.debug(f"  {code}: ì¶”ê°€í•  ë°ì´í„° ì—†ìŒ")
            return True
            
    except Exception as e:
        logger.error(f"  âœ— {code}: ê°±ì‹  ì‹¤íŒ¨ - {e}")
        return False


def run_data_update(max_stocks: int = MAX_STOCKS_PER_RUN) -> dict:
    """OHLCV ë°ì´í„° ìë™ ê°±ì‹ """
    print("=" * 50)
    print("ğŸ“Š OHLCV ë°ì´í„° ê°±ì‹  ì‹œì‘")
    print("=" * 50)
    
    today = date.today()
    
    if not is_market_open(today):
        print("íœ´ì¥ì¼ - ë°ì´í„° ê°±ì‹  ìŠ¤í‚µ")
        return {'updated': 0, 'failed': 0, 'skipped': 0}
    
    csv_files = list(DATA_DIR.glob("*.csv"))
    print(f"ì´ {len(csv_files)}ê°œ ì¢…ëª© íŒŒì¼ ë°œê²¬")
    
    stocks_to_update = []
    for csv_file in csv_files:
        code = csv_file.stem
        if not code.replace('K', '').isdigit():
            continue
        last_date = get_last_date_in_csv(csv_file)
        if last_date is None:
            continue
        if last_date < today:
            stocks_to_update.append((code, last_date))
    
    print(f"ê°±ì‹  í•„ìš”: {len(stocks_to_update)}ê°œ ì¢…ëª©")
    
    stocks_to_update.sort(key=lambda x: x[1])
    stocks_to_update = stocks_to_update[:max_stocks]
    
    results = {'updated': 0, 'failed': 0, 'skipped': 0}
    
    for i, (code, last_date) in enumerate(stocks_to_update, 1):
        days_behind = (today - last_date).days
        print(f"[{i}/{len(stocks_to_update)}] {code} - ë§ˆì§€ë§‰: {last_date} ({days_behind}ì¼ ì „)")
        
        success = update_single_stock(code, last_date, today)
        if success:
            results['updated'] += 1
        else:
            results['failed'] += 1
        
        time.sleep(API_DELAY)
    
    print("=" * 50)
    print(f"ğŸ“Š ë°ì´í„° ê°±ì‹  ì™„ë£Œ: ì„±ê³µ {results['updated']}, ì‹¤íŒ¨ {results['failed']}")
    print("=" * 50)
    
    return results


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    run_data_update(max_stocks=10)


# ============================================
# ê¸€ë¡œë²Œ ë°ì´í„° ê°±ì‹  (v5.4)
# ============================================

# ê¸€ë¡œë²Œ ì§€í‘œ ì‹¬ë³¼
GLOBAL_SYMBOLS = {
    'nasdaq': 'IXIC',      # ë‚˜ìŠ¤ë‹¥ ì¢…í•©
    'dow': 'DJI',          # ë‹¤ìš°ì¡´ìŠ¤
    'sp500': 'US500',      # S&P 500
    'usdkrw': 'USD/KRW',   # ì›/ë‹¬ëŸ¬ í™˜ìœ¨
    'kospi': 'KS11',       # ì½”ìŠ¤í”¼
    'kosdaq': 'KQ11',      # ì½”ìŠ¤ë‹¥
}


def update_global_data() -> dict:
    """ê¸€ë¡œë²Œ ì§€í‘œ ë°ì´í„° ê°±ì‹  (ë‚˜ìŠ¤ë‹¥, ë‹¤ìš°, S&P500, í™˜ìœ¨, ì½”ìŠ¤í”¼, ì½”ìŠ¤ë‹¥)
    
    Returns:
        ê°±ì‹  ê²°ê³¼ {'updated': int, 'failed': int}
    """
    try:
        import FinanceDataReader as fdr
    except ImportError:
        logger.error("FinanceDataReader ë¯¸ì„¤ì¹˜. pip install finance-datareader")
        return {'updated': 0, 'failed': len(GLOBAL_SYMBOLS)}
    
    print("=" * 50)
    print("ğŸŒ ê¸€ë¡œë²Œ ë°ì´í„° ê°±ì‹  ì‹œì‘")
    print("=" * 50)
    
    today = date.today()
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    GLOBAL_DIR.mkdir(parents=True, exist_ok=True)
    
    results = {'updated': 0, 'failed': 0}
    
    for name, symbol in GLOBAL_SYMBOLS.items():
        file_path = GLOBAL_DIR / f"{name}.csv"
        
        try:
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            if file_path.exists():
                df_existing = load_csv_with_date(file_path)
                if df_existing is not None and len(df_existing) > 0:
                    last_date = df_existing.index[-1].date()
                    
                    # ì´ë¯¸ ìµœì‹ ì´ë©´ ìŠ¤í‚µ
                    if last_date >= today - timedelta(days=1):
                        logger.debug(f"  {name}: ì´ë¯¸ ìµœì‹  ({last_date})")
                        results['updated'] += 1
                        continue
                    
                    # ë¶€ì¡±í•œ ê¸°ê°„ë§Œ ì¡°íšŒ
                    start_date = last_date + timedelta(days=1)
                else:
                    df_existing = None
                    start_date = date(2016, 6, 1)
            else:
                # ì‹ ê·œ: 2016ë…„ë¶€í„°
                df_existing = None
                start_date = date(2016, 6, 1)
            
            # ë°ì´í„° ì¡°íšŒ
            df_new = fdr.DataReader(symbol, start_date, today)
            
            if df_new is None or len(df_new) == 0:
                logger.warning(f"  {name}: ì‹ ê·œ ë°ì´í„° ì—†ìŒ")
                results['updated'] += 1
                continue
            
            # ì»¬ëŸ¼ ì •ë¦¬ (ì†Œë¬¸ì í†µì¼, date ì»¬ëŸ¼ ëª…ì‹œ)
            df_new = df_new[['Open', 'High', 'Low', 'Close', 'Volume']].copy()
            df_new.columns = ['open', 'high', 'low', 'close', 'volume']
            df_new.index.name = 'date'
            
            # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
            if df_existing is not None:
                # ê¸°ì¡´ íŒŒì¼ë„ ì†Œë¬¸ìë¡œ ì •ê·œí™”
                df_existing.columns = df_existing.columns.str.lower()
                df_existing.index.name = 'date'
                df_combined = pd.concat([df_existing, df_new])
                df_combined = df_combined[~df_combined.index.duplicated(keep='last')]
                df_combined.sort_index(inplace=True)
            else:
                df_combined = df_new
            
            # Aì•ˆ: í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì €ì¥
            keep_cols = ['open', 'high', 'low', 'close', 'volume']
            save_cols = [c for c in keep_cols if c in df_combined.columns]
            df_combined = df_combined[save_cols]
            
            # ì†Œìˆ˜ì  ì •ë¦¬ (OHLC 2ìë¦¬, volume ì •ìˆ˜)
            for col in ['open', 'high', 'low', 'close']:
                if col in df_combined.columns:
                    df_combined[col] = df_combined[col].round(2)
            if 'volume' in df_combined.columns:
                df_combined['volume'] = df_combined['volume'].clip(lower=0).astype('int64')
            
            # ì €ì¥ (date ì»¬ëŸ¼ ëª…ì‹œ)
            df_combined.to_csv(file_path, index_label='date')
            
            new_count = len(df_new)
            logger.info(f"  âœ“ {name}: {new_count}ì¼ ì¶”ê°€ (ë§ˆì§€ë§‰: {df_combined.index[-1].date()})")
            results['updated'] += 1
            
        except Exception as e:
            logger.error(f"  âœ— {name}: ê°±ì‹  ì‹¤íŒ¨ - {e}")
            results['failed'] += 1
    
    # global_merged.csv ê°±ì‹ 
    try:
        update_global_merged()
    except Exception as e:
        logger.warning(f"global_merged ê°±ì‹  ì‹¤íŒ¨: {e}")
    
    print("=" * 50)
    print(f"ğŸŒ ê¸€ë¡œë²Œ ë°ì´í„° ê°±ì‹  ì™„ë£Œ: ì„±ê³µ {results['updated']}, ì‹¤íŒ¨ {results['failed']}")
    print("=" * 50)
    
    return results


def update_global_merged():
    """ê¸€ë¡œë²Œ í†µí•© ë°ì´í„° ê°±ì‹  (global_merged.csv)"""
    
    # ì½”ìŠ¤í”¼ ê¸°ì¤€ (í•œêµ­ ê±°ë˜ì¼)
    kospi_path = GLOBAL_DIR / "kospi.csv"
    if not kospi_path.exists():
        logger.warning("kospi.csv ì—†ìŒ - global_merged ìŠ¤í‚µ")
        return
    
    kospi = pd.read_csv(kospi_path)
    
    # date ì»¬ëŸ¼ ì²˜ë¦¬
    if 'date' in kospi.columns:
        kospi['date'] = pd.to_datetime(kospi['date'])
        kospi = kospi.set_index('date')
    else:
        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œ
        first_col = kospi.columns[0]
        kospi[first_col] = pd.to_datetime(kospi[first_col])
        kospi = kospi.set_index(first_col)
    
    # ì»¬ëŸ¼ ì†Œë¬¸ì í†µì¼
    kospi.columns = kospi.columns.str.lower()
    
    # ê° ì§€í‘œ ë¡œë“œ ë° ë³‘í•©
    merged = pd.DataFrame(index=kospi.index)
    merged['date_kr'] = merged.index
    
    for name in GLOBAL_SYMBOLS.keys():
        file_path = GLOBAL_DIR / f"{name}.csv"
        if file_path.exists():
            df = pd.read_csv(file_path)
            
            # date ì»¬ëŸ¼ ì²˜ë¦¬
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df = df.set_index('date')
            else:
                first_col = df.columns[0]
                df[first_col] = pd.to_datetime(df[first_col])
                df = df.set_index(first_col)
            
            # ì»¬ëŸ¼ ì†Œë¬¸ì í†µì¼
            df.columns = df.columns.str.lower()
            
            # ë“±ë½ë¥  ê³„ì‚°
            df['change_pct'] = ((df['close'] / df['close'].shift(1)) - 1) * 100
            
            # í•œêµ­ ë‚ ì§œì— ë§ì¶° ë³‘í•© (ë¯¸êµ­ ë°ì´í„°ëŠ” +1ì¼ ë§¤í•‘)
            if name in ['nasdaq', 'dow', 'sp500', 'usdkrw']:
                # ë¯¸êµ­ ë°ì´í„°: ë‹¤ìŒ í•œêµ­ ì˜ì—…ì¼ì— ì˜í–¥
                df.index = df.index + pd.Timedelta(days=1)
            
            merged[f'{name}_close'] = df['close']
            merged[f'{name}_change_pct'] = df['change_pct']
    
    # ë‚˜ìŠ¤ë‹¥ íŠ¸ë Œë“œ ë¶„ë¥˜
    if 'nasdaq_change_pct' in merged.columns:
        merged['nasdaq_trend'] = merged['nasdaq_change_pct'].apply(
            lambda x: 'í­ë“±' if x >= 2 else 'ê¸‰ë“±' if x >= 1 else 'ìƒìŠ¹' if x > 0 
            else 'í•˜ë½' if x > -1 else 'ê¸‰ë½' if x > -2 else 'í­ë½' if pd.notna(x) else 'unknown'
        )
    
    # í™˜ìœ¨ íŠ¸ë Œë“œ ë¶„ë¥˜
    if 'usdkrw_change_pct' in merged.columns:
        merged['fx_trend'] = merged['usdkrw_change_pct'].apply(
            lambda x: 'ì›í™”ê°•ì„¸' if x <= -0.5 else 'ì•½ë³´í•©' if x < 0 
            else 'ê°•ë³´í•©' if x < 0.5 else 'ì›í™”ì•½ì„¸' if pd.notna(x) else 'unknown'
        )
    
    # NaN ì œê±°
    merged = merged.dropna(subset=['kospi_close'])
    
    # ì €ì¥ (date ì»¬ëŸ¼ ëª…ì‹œ)
    merged_path = GLOBAL_DIR / "global_merged.csv"
    merged.to_csv(merged_path, index_label='date')
    
    logger.info(f"  âœ“ global_merged.csv: {len(merged)}ì¼ ì €ì¥")


def run_full_data_update(max_stocks: int = MAX_STOCKS_PER_RUN) -> dict:
    """OHLCV + ê¸€ë¡œë²Œ ë°ì´í„° ì „ì²´ ê°±ì‹  (v5.4)
    
    Returns:
        {'ohlcv': dict, 'global': dict}
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š ì „ì²´ ë°ì´í„° ê°±ì‹  ì‹œì‘ (OHLCV + ê¸€ë¡œë²Œ)")
    print("=" * 60 + "\n")
    
    # 1. OHLCV ê°±ì‹ 
    ohlcv_result = run_data_update(max_stocks=max_stocks)
    
    # 2. ê¸€ë¡œë²Œ ë°ì´í„° ê°±ì‹ 
    global_result = update_global_data()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ì „ì²´ ë°ì´í„° ê°±ì‹  ì™„ë£Œ")
    print(f"   OHLCV: ì„±ê³µ {ohlcv_result['updated']}, ì‹¤íŒ¨ {ohlcv_result['failed']}")
    print(f"   ê¸€ë¡œë²Œ: ì„±ê³µ {global_result['updated']}, ì‹¤íŒ¨ {global_result['failed']}")
    print("=" * 60)
    
    return {
        'ohlcv': ohlcv_result,
        'global': global_result,
    }


# ============================================
# KIS OHLCV ìˆ˜ì§‘ (ì •ê·œì¥ ê¸°ì¤€)
# ============================================

# run_kis_data_update ì œê±° - run_data_updateë¡œ í†µí•© (v7.0)


def run_kis_data_update(days: int = 5) -> dict:
    """ë ˆê±°ì‹œ í˜¸í™˜ ë˜í¼ - v7.0ì—ì„œ run_data_updateë¡œ í†µí•©"""
    logger.warning("run_kis_data_updateëŠ” deprecatedì…ë‹ˆë‹¤. run_data_updateë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    return run_data_update(max_stocks=MAX_STOCKS_PER_RUN)