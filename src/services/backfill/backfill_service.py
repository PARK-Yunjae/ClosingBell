"""
ClosingBell v6.3.2 ë°±í•„ ì„œë¹„ìŠ¤

ê³¼ê±° ë°ì´í„° ë°±í•„:
- TOP5 20ì¼ ì¶”ì  ë°ì´í„°
- ìœ ëª©ë¯¼ ê³µë¶€ë²• (ìƒí•œê°€/ê±°ë˜ëŸ‰ì²œë§Œ) ë°ì´í„°

v6.3.2 ë³€ê²½ì‚¬í•­:
- TOP5 ì ìˆ˜ ê³„ì‚°ì„ ScoreCalculatorV5ë¡œ í†µì¼ (ì‹¤ì‹œê°„ê³¼ 100% ë™ì¼)
- ë” ì´ìƒ backfill/indicators.pyì˜ calculate_score()ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
- realtime ìš°ì„  ì •ì±…: ë°±í•„ì´ realtime ë°ì´í„°ë¥¼ ë®ì–´ì“°ì§€ ì•ŠìŒ
"""

import logging
from datetime import date, timedelta
from typing import List, Optional, Dict, Tuple
import pandas as pd

from src.config.backfill_config import BackfillConfig, get_backfill_config
from src.services.backfill.data_loader import (
    load_all_ohlcv,
    load_stock_mapping,
    get_trading_days,
    filter_stocks,
    load_global_index,
)
from src.services.backfill.indicators import (
    calculate_all_indicators,
    calculate_score,
    score_to_grade,
    calculate_global_adjustment,
)
from src.infrastructure.repository import (
    get_top5_history_repository,
    get_top5_prices_repository,
    get_nomad_candidates_repository,
)

logger = logging.getLogger(__name__)


class HistoricalBackfillService:
    """ê³¼ê±° ë°ì´í„° ë°±í•„ ì„œë¹„ìŠ¤"""
    
    def __init__(self, config: Optional[BackfillConfig] = None):
        self.config = config or get_backfill_config()
        self.stock_mapping = None
        self.ohlcv_data = None
        self.trading_days = None
        # v6.3.3: ê¸€ë¡œë²Œ ë°ì´í„° (ë‚˜ìŠ¤ë‹¥, í™˜ìœ¨)
        self.nasdaq_data = None
        self.usdkrw_data = None
    
    def load_data(
        self,
        start_date: Optional[date] = None,
        end_date: Optional[date] = None,
    ) -> bool:
        """ë°ì´í„° ë¡œë“œ
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        # ê¸°ë³¸ ë‚ ì§œ ì„¤ì •
        if end_date is None:
            end_date = date.today() - timedelta(days=0)  # ì–´ì œ
        
        if start_date is None:
            start_date = end_date - timedelta(days=60)
        
        logger.info(f"ë°ì´í„° ë¡œë“œ ì‹œì‘: {start_date} ~ {end_date}")
        
        # ì¢…ëª© ë§¤í•‘ ë¡œë“œ
        self.stock_mapping = load_stock_mapping(self.config)
        if self.stock_mapping.empty:
            logger.error("ì¢…ëª© ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        logger.info(f"ì¢…ëª© ë§¤í•‘ ë¡œë“œ: {len(self.stock_mapping)}ê°œ")
        
        # ê±°ë˜ì¼ ë¡œë“œ
        self.trading_days = get_trading_days(self.config, start_date, end_date)
        if not self.trading_days:
            logger.error("ê±°ë˜ì¼ ì¡°íšŒ ì‹¤íŒ¨")
            return False
        
        logger.info(f"ê±°ë˜ì¼: {len(self.trading_days)}ì¼")
        
        # OHLCV ë¡œë“œ (ë°±í•„ ê¸°ê°„ + 60ì¼ ì¶”ê°€ - ì§€í‘œ ê³„ì‚°ìš©)
        extended_start = start_date - timedelta(days=90)
        
        self.ohlcv_data = load_all_ohlcv(
            self.config,
            start_date=extended_start,
            end_date=end_date,
            num_workers=self.config.num_workers,
        )
        
        if not self.ohlcv_data:
            logger.error("OHLCV ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        logger.info(f"OHLCV ë¡œë“œ: {len(self.ohlcv_data)}ê°œ ì¢…ëª©")
        
        # v6.3.3: ê¸€ë¡œë²Œ ë°ì´í„° ë¡œë“œ (ë‚˜ìŠ¤ë‹¥, í™˜ìœ¨)
        self.nasdaq_data = load_global_index(self.config, 'NASDAQ')
        if self.nasdaq_data is not None:
            logger.info(f"ë‚˜ìŠ¤ë‹¥ ë¡œë“œ: {len(self.nasdaq_data)}ì¼")
        
        # USD/KRW íŒŒì¼ ì‹œë„
        self.usdkrw_data = load_global_index(self.config, 'USDKRW')
        if self.usdkrw_data is None:
            # ëŒ€ì²´ íŒŒì¼ëª… ì‹œë„
            for name in ['USD_KRW', 'usdkrw', 'usd_krw', 'FX']:
                self.usdkrw_data = load_global_index(self.config, name)
                if self.usdkrw_data is not None:
                    break
        
        if self.usdkrw_data is not None:
            logger.info(f"í™˜ìœ¨ ë¡œë“œ: {len(self.usdkrw_data)}ì¼")
        
        return True
    
    def _calculate_daily_scores(self, trade_date: date) -> pd.DataFrame:
        """íŠ¹ì • ë‚ ì§œì˜ ì „ì²´ ì¢…ëª© ì ìˆ˜ ê³„ì‚° (v6.3.3 í†µì¼)
        
        âš ï¸ ì¤‘ìš”: 
        1. TV200 ìŠ¤ëƒ…ìƒ·ì´ ìˆìœ¼ë©´ â†’ ìŠ¤ëƒ…ìƒ· ì½”ë“œë§Œ ì ìˆ˜ ê³„ì‚° (ìœ ë‹ˆë²„ìŠ¤ ì¼ì¹˜)
        2. ìŠ¤ëƒ…ìƒ· ì—†ìœ¼ë©´ â†’ ëª¨ë“  OHLCV + filter_stocks (fallback)
        3. ì ìˆ˜ ê³„ì‚°ì€ ScoreCalculatorV5 (ì‹¤ì‹œê°„ê³¼ 100% ë™ì¼)
        
        Args:
            trade_date: ê±°ë˜ì¼
            
        Returns:
            ì ìˆ˜ DataFrame
        """
        from src.domain.models import DailyPrice, StockData
        from src.domain.score_calculator import ScoreCalculatorV5
        from src.config.constants import MIN_DAILY_DATA_COUNT
        
        calculator = ScoreCalculatorV5()
        
        # v6.3.3: ê¸€ë¡œë²Œ ì¡°ì •ê°’ ê³„ì‚° (í•´ë‹¹ ë‚ ì§œ ê¸°ì¤€)
        global_adjustment = self._get_global_adjustment(trade_date)
        if global_adjustment != 0:
            logger.info(f"[{trade_date}] ê¸€ë¡œë²Œ ì¡°ì •: {global_adjustment:+d}ì ")
        
        # v6.3.3: TV200 ìŠ¤ëƒ…ìƒ· í™•ì¸ (ìœ ë‹ˆë²„ìŠ¤ ì†ŒìŠ¤ ì˜¤ë¸Œ íŠ¸ë£¨ìŠ¤)
        universe_codes = self._get_universe_codes(trade_date)
        use_snapshot = universe_codes is not None
        
        if use_snapshot:
            logger.info(f"[{trade_date}] TV200 ìŠ¤ëƒ…ìƒ· ì‚¬ìš©: {len(universe_codes)}ê°œ")
            target_codes = set(universe_codes)
        else:
            logger.info(f"[{trade_date}] TV200 ìŠ¤ëƒ…ìƒ· ì—†ìŒ â†’ OHLCV ê¸°ë°˜ í•„í„° ì‚¬ìš©")
            target_codes = set(self.ohlcv_data.keys())
        
        results = []
        
        # ì‹¤ì‹œê°„ê³¼ ë™ì¼í•œ ë£©ë°± ê¸¸ì´ (MIN_DAILY_DATA_COUNT + 10 = 30ë´‰)
        lookback_days = MIN_DAILY_DATA_COUNT + 10
        
        for code in target_codes:
            if code not in self.ohlcv_data:
                # ìŠ¤ëƒ…ìƒ·ì—ëŠ” ìˆì§€ë§Œ OHLCVê°€ ì—†ëŠ” ê²½ìš° (ë“œë¬¾)
                logger.debug(f"OHLCV ì—†ìŒ: {code}")
                continue
                
            df = self.ohlcv_data[code]
            
            try:
                # í•´ë‹¹ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°
                mask = df['date'].dt.date <= trade_date
                df_until = df[mask].copy()
                
                if len(df_until) < MIN_DAILY_DATA_COUNT:
                    continue
                
                # ë§ˆì§€ë§‰ í–‰ì´ í•´ë‹¹ ë‚ ì§œì¸ì§€ í™•ì¸
                if df_until.iloc[-1]['date'].date() != trade_date:
                    continue
                
                # ì‹¤ì‹œê°„ê³¼ ë™ì¼í•˜ê²Œ ìµœê·¼ 30ë´‰ë§Œ ì‚¬ìš©
                df_recent = df_until.tail(lookback_days)
                
                # DataFrame â†’ List[DailyPrice] ë³€í™˜
                daily_prices = self._convert_to_daily_prices(df_recent)
                
                if len(daily_prices) < MIN_DAILY_DATA_COUNT:
                    continue
                
                # ì¢…ëª©ëª…/ì—…ì¢… ì¡°íšŒ
                name_row = self.stock_mapping[self.stock_mapping['code'] == code]
                name = name_row['name'].iloc[0] if len(name_row) > 0 else code
                sector = name_row['sector'].iloc[0] if len(name_row) > 0 and 'sector' in self.stock_mapping.columns else None
                
                # ê±°ë˜ëŒ€ê¸ˆ ê³„ì‚° (ì–µì›)
                today_row = df_recent.iloc[-1]
                trading_value = today_row['close'] * today_row['volume'] / 100_000_000
                
                # StockData ìƒì„± (ì‹¤ì‹œê°„ê³¼ ë™ì¼í•œ êµ¬ì¡°)
                stock_data = StockData(
                    code=code,
                    name=name,
                    daily_prices=daily_prices,
                    current_price=int(today_row['close']),
                    trading_value=trading_value,
                )
                
                # ğŸ”¥ í•µì‹¬: ScoreCalculatorV5ë¡œ ì ìˆ˜ ê³„ì‚° (ì‹¤ì‹œê°„ê³¼ 100% ë™ì¼)
                score_result = calculator.calculate_single_score(stock_data)
                
                if score_result is None:
                    continue
                
                # ê¸€ë¡œë²Œ ì¡°ì • ì ìš©
                final_score = min(100.0, score_result.score_total + global_adjustment)
                
                # ë“±ê¸‰ ì¬ê³„ì‚° (ê¸€ë¡œë²Œ ì¡°ì • ë°˜ì˜)
                from src.domain.score_calculator import get_grade
                grade = get_grade(final_score)
                
                results.append({
                    'date': trade_date,
                    'code': code,
                    'name': name,
                    'close': int(today_row['close']),
                    'change_rate': score_result.change_rate,
                    'trading_value': trading_value,
                    'volume': int(today_row['volume']),
                    'score': final_score,
                    'grade': grade.value,
                    # ScoreCalculatorV5ì—ì„œ ê³„ì‚°ëœ ì§€í‘œê°’ ì‚¬ìš©
                    'cci': score_result.score_detail.raw_cci,
                    'rsi': score_result.score_detail.raw_rsi,  # v6.5: RSI ì €ì¥
                    'disparity_20': score_result.score_detail.raw_distance,
                    'consecutive_up': score_result.score_detail.raw_consec_days,
                    'volume_ratio_5': score_result.score_detail.raw_volume_ratio,
                    # v6.5.2: sector ì¶”ê°€
                    'sector': sector,
                })
                
            except Exception as e:
                logger.debug(f"ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨ {code}: {e}")
                continue
        
        df_result = pd.DataFrame(results)
        
        if len(df_result) > 0:
            before_count = len(df_result)
            before_codes = set(df_result['code'].tolist())
            
            # v6.3.3: ìŠ¤ëƒ…ìƒ· ì‚¬ìš© ì‹œ filter_stocks ìŠ¤í‚µ (ì´ë¯¸ í•„í„°ëœ ìœ ë‹ˆë²„ìŠ¤)
            if not use_snapshot:
                # ìŠ¤ëƒ…ìƒ· ì—†ì„ ë•Œë§Œ í•„í„°ë§
                df_result = filter_stocks(df_result, self.config, self.stock_mapping)
                
                after_count = len(df_result)
                after_codes = set(df_result['code'].tolist()) if len(df_result) > 0 else set()
                
                logger.info(f"[{trade_date}] í•„í„°: {before_count}ê°œ â†’ {after_count}ê°œ (ì œì™¸: {before_count - after_count}ê°œ)")
                
                # ìƒì„¸ ì €ì¥ (ì²« ë‚ ë§Œ)
                if hasattr(self, '_first_day_logged') is False or not self._first_day_logged:
                    self._save_backfill_filter_result(trade_date, before_codes, after_codes, df_result)
                    self._first_day_logged = True
            else:
                logger.info(f"[{trade_date}] ìŠ¤ëƒ…ìƒ· ì‚¬ìš© â†’ í•„í„° ìŠ¤í‚µ: {before_count}ê°œ")
        
        return df_result
    
    def _get_universe_codes(self, trade_date: date) -> Optional[List[str]]:
        """í•´ë‹¹ ë‚ ì§œì˜ TV200 ìœ ë‹ˆë²„ìŠ¤ ì½”ë“œ ì¡°íšŒ (v6.3.3)
        
        Returns:
            ìŠ¤ëƒ…ìƒ·ì´ ìˆìœ¼ë©´ ì½”ë“œ ë¦¬ìŠ¤íŠ¸, ì—†ìœ¼ë©´ None
        """
        try:
            from src.infrastructure.repository import get_tv200_snapshot_repository
            snapshot_repo = get_tv200_snapshot_repository()
            
            date_str = trade_date.isoformat()
            codes = snapshot_repo.get_codes_for_date(date_str, filter_stage='after')
            
            if codes:
                return codes
            
            # JSON íŒŒì¼ fallback (ìŠ¤ëƒ…ìƒ·ì´ ì—†ëŠ” ê²½ìš°)
            import json
            from pathlib import Path
            
            json_path = Path(f"logs/tv200_{date_str}_after_filter.json")
            if json_path.exists():
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if 'stocks' in data:
                        codes = [s['code'] for s in data['stocks']]
                        logger.info(f"[{trade_date}] JSON ìŠ¤ëƒ…ìƒ· ì‚¬ìš©: {len(codes)}ê°œ")
                        return codes
            
            return None
            
        except Exception as e:
            logger.debug(f"TV200 ìŠ¤ëƒ…ìƒ· ì¡°íšŒ ì‹¤íŒ¨ {trade_date}: {e}")
            return None
    
    def _convert_to_daily_prices(self, df: pd.DataFrame) -> List:
        """DataFrameì„ List[DailyPrice]ë¡œ ë³€í™˜ (v6.3.2)
        
        ì‹¤ì‹œê°„ kis_client.get_daily_prices()ì™€ ë™ì¼í•œ í˜•íƒœë¡œ ë³€í™˜
        
        Args:
            df: OHLCV DataFrame (ì˜¤ë˜ëœ ìˆœ ì •ë ¬)
            
        Returns:
            List[DailyPrice]
        """
        from src.domain.models import DailyPrice
        
        daily_prices = []
        
        for _, row in df.iterrows():
            try:
                # ë‚ ì§œ ë³€í™˜
                row_date = row['date']
                if isinstance(row_date, pd.Timestamp):
                    row_date = row_date.date()
                elif isinstance(row_date, str):
                    row_date = pd.to_datetime(row_date).date()
                
                # ê±°ë˜ëŒ€ê¸ˆ ê³„ì‚°
                trading_value = row['close'] * row['volume']  # ì› ë‹¨ìœ„
                
                daily_price = DailyPrice(
                    date=row_date,
                    open=int(row['open']),
                    high=int(row['high']),
                    low=int(row['low']),
                    close=int(row['close']),
                    volume=int(row['volume']),
                    trading_value=trading_value,
                )
                daily_prices.append(daily_price)
                
            except Exception as e:
                logger.debug(f"DailyPrice ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue
        
        return daily_prices
    
    def _get_global_adjustment(self, trade_date: date) -> int:
        """í•´ë‹¹ ë‚ ì§œì˜ ê¸€ë¡œë²Œ ì¡°ì •ê°’ ê³„ì‚° (v6.3.3)
        
        Args:
            trade_date: ê±°ë˜ì¼
            
        Returns:
            ì ìˆ˜ ì¡°ì •ê°’ (0, 3, 5)
        """
        nasdaq_change = None
        usdkrw_change = None
        
        # ë‚˜ìŠ¤ë‹¥ ì „ì¼ ëŒ€ë¹„ ë³€í™”ìœ¨ ì¡°íšŒ
        # í•œêµ­ì‹œê°„ ê¸°ì¤€ ê±°ë˜ì¼ ì „ë‚ ì˜ ë¯¸êµ­ì¥ ë§ˆê° ë°ì´í„°
        if self.nasdaq_data is not None:
            # ê±°ë˜ì¼ ë˜ëŠ” ê·¸ ì „ë‚ ì˜ ë°ì´í„° ì°¾ê¸°
            mask = self.nasdaq_data['date'].dt.date <= trade_date
            nasdaq_until = self.nasdaq_data[mask]
            if len(nasdaq_until) >= 1:
                nasdaq_change = nasdaq_until.iloc[-1]['change_rate']
        
        # í™˜ìœ¨ ë³€í™”ìœ¨ ì¡°íšŒ
        if self.usdkrw_data is not None:
            mask = self.usdkrw_data['date'].dt.date <= trade_date
            usdkrw_until = self.usdkrw_data[mask]
            if len(usdkrw_until) >= 1:
                usdkrw_change = usdkrw_until.iloc[-1]['change_rate']
        
        return calculate_global_adjustment(nasdaq_change, usdkrw_change)
    
    def _save_backfill_filter_result(self, trade_date, before_codes, after_codes, df_result):
        """ë°±í•„ í•„í„° ê²°ê³¼ ì €ì¥ (v6.3.2)"""
        import json
        from pathlib import Path
        
        try:
            filepath = Path(f"logs/backfill_{trade_date}_filter.json")
            filepath.parent.mkdir(exist_ok=True)
            
            # TOP10 ì •ë³´
            top10 = []
            if len(df_result) > 0:
                df_sorted = df_result.sort_values('score', ascending=False).head(10)
                for _, row in df_sorted.iterrows():
                    top10.append({
                        'code': row['code'],
                        'name': row['name'],
                        'score': round(row['score'], 2),
                        'change_rate': round(row.get('change_rate', 0), 2),
                        'trading_value': round(row.get('trading_value', 0), 1),
                    })
            
            data = {
                'date': str(trade_date),
                'before_filter': len(before_codes),
                'after_filter': len(after_codes),
                'filtered_out_count': len(before_codes - after_codes),
                'after_codes': sorted(list(after_codes)),
                'top10': top10,
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"ë°±í•„ í•„í„° ê²°ê³¼ ì €ì¥: {filepath}")
            
        except Exception as e:
            logger.warning(f"ë°±í•„ í•„í„° ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def backfill_top5(
        self,
        days: int = 20,
        end_date: Optional[date] = None,
        dry_run: bool = False,
    ) -> Dict[str, int]:
        """TOP5 ë°±í•„
        
        Args:
            days: ë°±í•„ ì¼ìˆ˜
            end_date: ì¢…ë£Œ ë‚ ì§œ
            dry_run: Trueë©´ DB ì €ì¥ ì•ˆ í•¨
            
        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if end_date is None:
            end_date = date.today() - timedelta(days=0)
        
        start_date = end_date - timedelta(days=days + 30)  # ì—¬ìœ  ì¶”ê°€
        
        # ë°ì´í„° ë¡œë“œ
        if not self.load_data(start_date, end_date):
            return {'error': 'data_load_failed'}
        
        # ë°±í•„ ëŒ€ìƒ ê±°ë˜ì¼
        target_days = [d for d in self.trading_days if d <= end_date][-days:]
        
        logger.info(f"TOP5 ë°±í•„ ì‹œì‘: {len(target_days)}ì¼")
        
        stats = {
            'total_days': len(target_days),
            'processed_days': 0,
            'top5_saved': 0,
            'prices_saved': 0,
        }
        
        # Repository
        history_repo = get_top5_history_repository()
        prices_repo = get_top5_prices_repository()
        
        for i, trade_date in enumerate(target_days):
            logger.info(f"[{i+1}/{len(target_days)}] {trade_date} ì²˜ë¦¬ ì¤‘...")
            
            # ì ìˆ˜ ê³„ì‚°
            df_scores = self._calculate_daily_scores(trade_date)
            
            if len(df_scores) == 0:
                logger.warning(f"{trade_date}: ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨")
                continue
            
            # TOP5 ì¶”ì¶œ (ì ìˆ˜ ê¸°ì¤€ ì •ë ¬)
            df_scores = df_scores.sort_values('score', ascending=False)
            top5 = df_scores.head(5)
            
            for rank, (_, row) in enumerate(top5.iterrows(), 1):
                if dry_run:
                    logger.info(f"  #{rank} {row['name']} ({row['code']}) - {row['score']:.1f}ì  {row['grade']}ë“±ê¸‰")
                    continue
                
                # DB ì €ì¥
                history_data = {
                    'screen_date': trade_date.isoformat(),
                    'rank': rank,
                    'stock_code': row['code'],
                    'stock_name': row['name'],
                    'screen_price': row['close'],
                    'screen_score': row['score'],
                    'grade': row['grade'],
                    'cci': row.get('cci'),
                    'rsi': row.get('rsi'),
                    'change_rate': row.get('change_rate'),
                    'disparity_20': row.get('disparity_20'),
                    'consecutive_up': row.get('consecutive_up', 0),
                    'volume_ratio_5': row.get('volume_ratio_5'),  # v6.3.2: ì´ë¯¸ 151ë²ˆì¤„ì—ì„œ 19ì¼ í‰ê· ê°’ìœ¼ë¡œ ì €ì¥ë¨
                    'data_source': 'backfill',
                    # v6.3.1: ê±°ë˜ëŒ€ê¸ˆ/ê±°ë˜ëŸ‰
                    'trading_value': row.get('trading_value'),
                    'volume': row.get('volume'),
                    # v6.5.2: sector ì¶”ê°€ (stock_mappingì—ì„œ ì¡°íšŒ)
                    'sector': row.get('sector'),
                    'sector_rank': None,  # ë°±í•„ì—ì„œëŠ” ìˆœìœ„ ê³„ì‚° ì•ˆ í•¨
                    'is_leading_sector': 0,
                }
                
                # v6.3.2: realtime ìš°ì„  ì •ì±… - realtimeì´ ìˆìœ¼ë©´ ë®ì–´ì“°ì§€ ì•ŠìŒ
                history_id = history_repo.upsert_backfill_safe(history_data)
                
                if history_id is None:
                    # realtime ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìŠ¤í‚µë¨
                    logger.debug(f"realtime ì¡´ì¬ë¡œ ìŠ¤í‚µ: {trade_date} {row['code']}")
                    continue
                
                stats['top5_saved'] += 1
                
                # D+1 ~ D+20 ê°€ê²© ì €ì¥
                future_days = [d for d in self.trading_days if d > trade_date][:20]
                
                for days_after, future_date in enumerate(future_days, 1):
                    # í•´ë‹¹ ë‚ ì§œì˜ ê°€ê²©
                    code = row['code']
                    if code not in self.ohlcv_data:
                        continue
                    
                    df_stock = self.ohlcv_data[code]
                    mask = df_stock['date'].dt.date == future_date
                    df_day = df_stock[mask]
                    
                    if len(df_day) == 0:
                        continue
                    
                    day_data = df_day.iloc[0]
                    screen_price = row['close']
                    
                    price_data = {
                        'top5_history_id': history_id,
                        'trade_date': future_date.isoformat(),
                        'days_after': days_after,
                        'open_price': int(day_data['open']),
                        'high_price': int(day_data['high']),
                        'low_price': int(day_data['low']),
                        'close_price': int(day_data['close']),
                        'volume': int(day_data['volume']),
                        'return_from_screen': (day_data['close'] - screen_price) / screen_price * 100,
                        'gap_rate': (day_data['open'] - screen_price) / screen_price * 100,
                        'high_return': (day_data['high'] - screen_price) / screen_price * 100,
                        'low_return': (day_data['low'] - screen_price) / screen_price * 100,
                        'data_source': 'backfill',
                    }
                    
                    prices_repo.insert(price_data)
                    stats['prices_saved'] += 1
                
                # ì¶”ì  ìƒíƒœ ì—…ë°ì´íŠ¸
                if len(future_days) >= 20:
                    history_repo.update_status(history_id, 'completed')
                    history_repo.update_tracking_days(history_id, len(future_days), future_days[-1].isoformat())
            
            stats['processed_days'] += 1
        
        logger.info(f"TOP5 ë°±í•„ ì™„ë£Œ: {stats}")
        return stats
    
    def backfill_nomad(
        self,
        days: int = 20,
        end_date: Optional[date] = None,
        dry_run: bool = False,
    ) -> Dict[str, int]:
        """ìœ ëª©ë¯¼ ê³µë¶€ë²• ë°±í•„ (ìƒí•œê°€/ê±°ë˜ëŸ‰ì²œë§Œ)
        
        Args:
            days: ë°±í•„ ì¼ìˆ˜
            end_date: ì¢…ë£Œ ë‚ ì§œ
            dry_run: Trueë©´ DB ì €ì¥ ì•ˆ í•¨
            
        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if end_date is None:
            end_date = date.today() - timedelta(days=0)
        
        start_date = end_date - timedelta(days=days + 30)
        
        # ë°ì´í„° ë¡œë“œ
        if not self.load_data(start_date, end_date):
            return {'error': 'data_load_failed'}
        
        target_days = [d for d in self.trading_days if d <= end_date][-days:]
        
        logger.info(f"ìœ ëª©ë¯¼ ë°±í•„ ì‹œì‘: {len(target_days)}ì¼")
        
        stats = {
            'total_days': len(target_days),
            'processed_days': 0,
            'limit_up': 0,
            'volume_explosion': 0,
        }
        
        # Repository
        nomad_repo = get_nomad_candidates_repository()
        
        for i, trade_date in enumerate(target_days):
            logger.info(f"[{i+1}/{len(target_days)}] {trade_date} ìœ ëª©ë¯¼ ì²˜ë¦¬ ì¤‘...")
            
            candidates = []
            
            for code, df in self.ohlcv_data.items():
                # í•´ë‹¹ ë‚ ì§œ ë°ì´í„°
                mask = df['date'].dt.date == trade_date
                df_day = df[mask]
                
                if len(df_day) == 0:
                    continue
                
                row = df_day.iloc[0]
                
                # ë“±ë½ë¥  ê³„ì‚°
                prev_mask = df['date'].dt.date < trade_date
                df_prev = df[prev_mask]
                
                if len(df_prev) == 0:
                    continue
                
                prev_close = df_prev.iloc[-1]['close']
                change_rate = (row['close'] - prev_close) / prev_close * 100
                
                # ê±°ë˜ëŒ€ê¸ˆ ê³„ì‚°
                trading_value = row['close'] * row['volume'] / 100_000_000
                
                # ìƒí•œê°€ í™•ì¸ (29.5% ì´ìƒ)
                is_limit_up = change_rate >= self.config.limit_up_threshold
                
                # ê±°ë˜ëŸ‰ì²œë§Œ í™•ì¸ (1000ë§Œì£¼ ì´ìƒ)
                is_volume_explosion = row['volume'] >= self.config.volume_explosion_shares
                
                if not (is_limit_up or is_volume_explosion):
                    continue
                
                # ì¢…ëª©ëª… ì¡°íšŒ
                name_row = self.stock_mapping[self.stock_mapping['code'] == code]
                name = name_row['name'].iloc[0] if len(name_row) > 0 else code
                
                # ETF ë“± ì œì™¸
                skip = False
                for pattern in self.config.exclude_patterns:
                    if pattern.lower() in name.lower():
                        skip = True
                        break
                
                if skip:
                    continue
                
                # ì‚¬ìœ  ê²°ì •
                if is_limit_up and is_volume_explosion:
                    reason = 'ìƒí•œê°€+ê±°ë˜ëŸ‰'
                elif is_limit_up:
                    reason = 'ìƒí•œê°€'
                else:
                    reason = 'ê±°ë˜ëŸ‰ì²œë§Œ'
                
                candidates.append({
                    'study_date': trade_date.isoformat(),
                    'stock_code': code,
                    'stock_name': name,
                    'reason_flag': reason,
                    'close_price': int(row['close']),
                    'change_rate': change_rate,
                    'volume': int(row['volume']),
                    'trading_value': trading_value,
                    'data_source': 'backfill',
                })
                
                if is_limit_up:
                    stats['limit_up'] += 1
                if is_volume_explosion:
                    stats['volume_explosion'] += 1
            
            # DB ì €ì¥
            if not dry_run:
                for candidate in candidates:
                    nomad_repo.upsert(candidate)
            else:
                for c in candidates:
                    logger.info(f"  {c['reason_flag']}: {c['stock_name']} ({c['stock_code']}) +{c['change_rate']:.1f}%")
            
            stats['processed_days'] += 1
        
        logger.info(f"ìœ ëª©ë¯¼ ë°±í•„ ì™„ë£Œ: {stats}")
        return stats
    
    def auto_fill_missing(
        self,
        days: int = 30,
    ) -> Dict[str, int]:
        """ëˆ„ë½ ë°ì´í„° ìë™ ì±„ìš°ê¸°
        
        ìµœê·¼ Nì¼ ì¤‘ ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œ ìë™ ë°±í•„
        """
        end_date = date.today() - timedelta(days=0)
        start_date = end_date - timedelta(days=days)
        
        # ê±°ë˜ì¼ ì¡°íšŒ
        self.trading_days = get_trading_days(self.config, start_date, end_date)
        
        # ê¸°ì¡´ ë°ì´í„° ë‚ ì§œ ì¡°íšŒ
        history_repo = get_top5_history_repository()
        existing_dates = set(history_repo.get_dates_with_data(days))
        
        # ëˆ„ë½ ë‚ ì§œ
        missing_dates = [d for d in self.trading_days if d.isoformat() not in existing_dates]
        
        if not missing_dates:
            logger.info("ëˆ„ë½ ë°ì´í„° ì—†ìŒ")
            return {'missing': 0}
        
        logger.info(f"ëˆ„ë½ ë°ì´í„° ë°œê²¬: {len(missing_dates)}ì¼")
        
        # ëˆ„ë½ ë‚ ì§œ ë°±í•„
        stats = {
            'missing': len(missing_dates),
            'top5_filled': 0,
            'nomad_filled': 0,
        }
        
        for missing_date in missing_dates:
            logger.info(f"ìë™ ì±„ìš°ê¸°: {missing_date}")
            
            # TOP5 ë°±í•„
            self.backfill_top5(days=1, end_date=missing_date)
            stats['top5_filled'] += 1
            
            # ìœ ëª©ë¯¼ ë°±í•„
            self.backfill_nomad(days=1, end_date=missing_date)
            stats['nomad_filled'] += 1
        
        return stats


# í¸ì˜ í•¨ìˆ˜
def backfill_top5(days: int = 20, dry_run: bool = False) -> Dict[str, int]:
    """TOP5 ë°±í•„ í¸ì˜ í•¨ìˆ˜"""
    service = HistoricalBackfillService()
    return service.backfill_top5(days=days, dry_run=dry_run)


def backfill_nomad(days: int = 20, dry_run: bool = False) -> Dict[str, int]:
    """ìœ ëª©ë¯¼ ë°±í•„ í¸ì˜ í•¨ìˆ˜"""
    service = HistoricalBackfillService()
    return service.backfill_nomad(days=days, dry_run=dry_run)


def auto_fill_missing(days: int = 30) -> Dict[str, int]:
    """ìë™ ì±„ìš°ê¸° í¸ì˜ í•¨ìˆ˜"""
    service = HistoricalBackfillService()
    return service.auto_fill_missing(days=days)